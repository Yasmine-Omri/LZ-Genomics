-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, False, remove, 0.0, depth, 48, 3.830, 79.87
1, 0.1, False, remove, 0.0, entropy, 48, 6.538, 80.95
1, 0.5, False, remove, 0.0, depth, 48, 9.295, 82.61
1, 0.5, False, remove, 0.0, entropy, 48, 12.048, 83.90
1, 0.33, False, remove, 0.0, depth, 48, 14.754, 81.79
1, 0.33, False, remove, 0.0, entropy, 48, 17.485, 82.95
1, 0.75, False, remove, 0.0, depth, 48, 20.213, 83.10
1, 0.75, False, remove, 0.0, entropy, 48, 22.912, 84.51
1, 1.0, False, remove, 0.0, depth, 48, 25.612, 83.44
1, 1.0, False, remove, 0.0, entropy, 48, 28.344, 85.13
1, 3.0, False, remove, 0.0, depth, 48, 31.021, 84.63
1, 3.0, False, remove, 0.0, entropy, 48, 33.770, 85.86
1, 5.0, False, remove, 0.0, depth, 48, 36.488, 84.80
1, 5.0, False, remove, 0.0, entropy, 48, 39.203, 85.92
3, 0.1, False, remove, 0.0, depth, 48, 5.232, 80.10
3, 0.1, False, remove, 0.0, entropy, 48, 7.981, 81.10
3, 0.5, False, remove, 0.0, depth, 48, 10.736, 82.68
3, 0.5, False, remove, 0.0, entropy, 48, 13.540, 83.69
3, 0.33, False, remove, 0.0, depth, 48, 16.295, 82.15
3, 0.33, False, remove, 0.0, entropy, 48, 19.080, 82.88
3, 0.75, False, remove, 0.0, depth, 48, 21.839, 83.32
3, 0.75, False, remove, 0.0, entropy, 48, 24.596, 84.23
3, 1.0, False, remove, 0.0, depth, 48, 27.343, 83.90
3, 1.0, False, remove, 0.0, entropy, 48, 30.127, 84.97
3, 3.0, False, remove, 0.0, depth, 48, 32.875, 85.10
3, 3.0, False, remove, 0.0, entropy, 48, 35.651, 86.52
3, 5.0, False, remove, 0.0, depth, 48, 38.390, 85.77
3, 5.0, False, remove, 0.0, entropy, 48, 41.148, 86.51
5, 0.1, False, remove, 0.0, depth, 48, 5.390, 80.16
5, 0.1, False, remove, 0.0, entropy, 48, 8.147, 81.29
5, 0.5, False, remove, 0.0, depth, 48, 10.946, 82.61
5, 0.5, False, remove, 0.0, entropy, 48, 13.743, 83.51
5, 0.33, False, remove, 0.0, depth, 48, 16.490, 81.73
5, 0.33, False, remove, 0.0, entropy, 48, 19.275, 82.77
5, 0.75, False, remove, 0.0, depth, 48, 22.049, 83.13
5, 0.75, False, remove, 0.0, entropy, 48, 24.813, 84.49
5, 1.0, False, remove, 0.0, depth, 48, 27.578, 83.56
5, 1.0, False, remove, 0.0, entropy, 48, 30.378, 84.82
5, 3.0, False, remove, 0.0, depth, 48, 33.130, 85.09
5, 3.0, False, remove, 0.0, entropy, 48, 35.916, 86.39
5, 5.0, False, remove, 0.0, depth, 48, 38.661, 85.53
5, 5.0, False, remove, 0.0, entropy, 48, 41.443, 86.67
7, 0.1, False, remove, 0.0, depth, 48, 5.405, 80.31
7, 0.1, False, remove, 0.0, entropy, 48, 8.163, 80.99
7, 0.5, False, remove, 0.0, depth, 48, 10.974, 82.25
7, 0.5, False, remove, 0.0, entropy, 48, 13.782, 83.42
7, 0.33, False, remove, 0.0, depth, 48, 16.550, 81.72
7, 0.33, False, remove, 0.0, entropy, 48, 19.368, 82.83
7, 0.75, False, remove, 0.0, depth, 48, 22.136, 82.85
7, 0.75, False, remove, 0.0, entropy, 48, 24.923, 84.24
7, 1.0, False, remove, 0.0, depth, 48, 27.714, 83.32
7, 1.0, False, remove, 0.0, entropy, 48, 30.521, 84.73
7, 3.0, False, remove, 0.0, depth, 48, 33.268, 84.80
7, 3.0, False, remove, 0.0, entropy, 48, 36.065, 86.15
7, 5.0, False, remove, 0.0, depth, 48, 38.825, 85.40
7, 5.0, False, remove, 0.0, entropy, 48, 41.630, 86.26
10, 0.1, False, remove, 0.0, depth, 48, 7.035, 80.64
10, 0.1, False, remove, 0.0, entropy, 48, 9.862, 81.53
10, 0.5, False, remove, 0.0, depth, 48, 12.674, 82.49
10, 0.5, False, remove, 0.0, entropy, 48, 15.514, 83.85
10, 0.33, False, remove, 0.0, depth, 48, 18.326, 81.87
10, 0.33, False, remove, 0.0, entropy, 48, 21.194, 82.95
10, 0.75, False, remove, 0.0, depth, 48, 24.051, 82.98
10, 0.75, False, remove, 0.0, entropy, 48, 26.897, 84.66
10, 1.0, False, remove, 0.0, depth, 48, 29.736, 83.47
10, 1.0, False, remove, 0.0, entropy, 48, 32.553, 84.77
10, 3.0, False, remove, 0.0, depth, 48, 35.378, 85.16
10, 3.0, False, remove, 0.0, entropy, 48, 38.227, 85.99
10, 5.0, False, remove, 0.0, depth, 48, 41.020, 85.78
10, 5.0, False, remove, 0.0, entropy, 48, 43.857, 86.57
1, 0.1, False, remove, 0.25, depth, 48, 3.853, 78.40
1, 0.1, False, remove, 0.25, entropy, 48, 6.626, 79.27
1, 0.5, False, remove, 0.25, depth, 48, 9.349, 80.99
1, 0.5, False, remove, 0.25, entropy, 48, 12.081, 82.82
1, 0.33, False, remove, 0.25, depth, 48, 14.833, 80.39
1, 0.33, False, remove, 0.25, entropy, 48, 17.590, 81.93
1, 0.75, False, remove, 0.25, depth, 48, 20.346, 81.70
1, 0.75, False, remove, 0.25, entropy, 48, 23.104, 83.99
1, 1.0, False, remove, 0.25, depth, 48, 25.803, 82.34
1, 1.0, False, remove, 0.25, entropy, 48, 28.574, 84.30
1, 3.0, False, remove, 0.25, depth, 48, 31.308, 83.91
1, 3.0, False, remove, 0.25, entropy, 48, 34.005, 85.59
1, 5.0, False, remove, 0.25, depth, 48, 36.768, 83.83
1, 5.0, False, remove, 0.25, entropy, 48, 39.546, 85.57
3, 0.1, False, remove, 0.25, depth, 48, 5.200, 79.21
3, 0.1, False, remove, 0.25, entropy, 48, 7.977, 80.47
3, 0.5, False, remove, 0.25, depth, 48, 10.697, 81.70
3, 0.5, False, remove, 0.25, entropy, 48, 13.487, 83.59
3, 0.33, False, remove, 0.25, depth, 48, 16.266, 81.07
3, 0.33, False, remove, 0.25, entropy, 48, 19.042, 82.45
3, 0.75, False, remove, 0.25, depth, 48, 21.818, 82.62
3, 0.75, False, remove, 0.25, entropy, 48, 24.627, 84.45
3, 1.0, False, remove, 0.25, depth, 48, 27.371, 83.11
3, 1.0, False, remove, 0.25, entropy, 48, 30.157, 84.76
3, 3.0, False, remove, 0.25, depth, 48, 32.912, 84.89
3, 3.0, False, remove, 0.25, entropy, 48, 35.677, 86.18
3, 5.0, False, remove, 0.25, depth, 48, 38.465, 85.17
3, 5.0, False, remove, 0.25, entropy, 48, 41.283, 86.23
5, 0.1, False, remove, 0.25, depth, 48, 5.457, 79.96
5, 0.1, False, remove, 0.25, entropy, 48, 8.295, 80.99
5, 0.5, False, remove, 0.25, depth, 48, 11.078, 82.45
5, 0.5, False, remove, 0.25, entropy, 48, 13.888, 83.56
5, 0.33, False, remove, 0.25, depth, 48, 16.683, 81.72
5, 0.33, False, remove, 0.25, entropy, 48, 19.445, 82.74
5, 0.75, False, remove, 0.25, depth, 48, 22.268, 83.05
5, 0.75, False, remove, 0.25, entropy, 48, 25.086, 84.55
5, 1.0, False, remove, 0.25, depth, 48, 27.864, 83.41
5, 1.0, False, remove, 0.25, entropy, 48, 30.677, 84.95
5, 3.0, False, remove, 0.25, depth, 48, 33.452, 84.94
5, 3.0, False, remove, 0.25, entropy, 48, 36.222, 86.42
5, 5.0, False, remove, 0.25, depth, 48, 39.031, 85.53
5, 5.0, False, remove, 0.25, entropy, 48, 41.823, 86.69
7, 0.1, False, remove, 0.25, depth, 48, 5.470, 80.28
7, 0.1, False, remove, 0.25, entropy, 48, 8.303, 81.25
7, 0.5, False, remove, 0.25, depth, 48, 11.075, 82.22
7, 0.5, False, remove, 0.25, entropy, 48, 13.889, 83.60
7, 0.33, False, remove, 0.25, depth, 48, 16.691, 81.72
7, 0.33, False, remove, 0.25, entropy, 48, 19.472, 82.97
7, 0.75, False, remove, 0.25, depth, 48, 22.279, 82.67
7, 0.75, False, remove, 0.25, entropy, 48, 25.098, 84.18
7, 1.0, False, remove, 0.25, depth, 48, 27.853, 83.14
7, 1.0, False, remove, 0.25, entropy, 48, 30.688, 84.88
7, 3.0, False, remove, 0.25, depth, 48, 33.478, 84.63
7, 3.0, False, remove, 0.25, entropy, 48, 36.259, 86.09
7, 5.0, False, remove, 0.25, depth, 48, 39.068, 85.22
7, 5.0, False, remove, 0.25, entropy, 48, 41.863, 86.83
10, 0.1, False, remove, 0.25, depth, 48, 7.068, 80.04
10, 0.1, False, remove, 0.25, entropy, 48, 9.874, 81.23
10, 0.5, False, remove, 0.25, depth, 48, 12.654, 82.49
10, 0.5, False, remove, 0.25, entropy, 48, 15.469, 83.53
10, 0.33, False, remove, 0.25, depth, 48, 18.282, 81.56
10, 0.33, False, remove, 0.25, entropy, 48, 21.094, 82.85
10, 0.75, False, remove, 0.25, depth, 48, 23.909, 83.22
10, 0.75, False, remove, 0.25, entropy, 48, 26.747, 84.23
10, 1.0, False, remove, 0.25, depth, 48, 29.532, 83.53
10, 1.0, False, remove, 0.25, entropy, 48, 32.356, 84.80
10, 3.0, False, remove, 0.25, depth, 48, 35.138, 85.14
10, 3.0, False, remove, 0.25, entropy, 48, 37.968, 86.11
10, 5.0, False, remove, 0.25, depth, 48, 40.759, 85.20
10, 5.0, False, remove, 0.25, entropy, 48, 43.529, 86.73
1, 0.1, False, remove, 0.1, depth, 48, 3.868, 79.05
1, 0.1, False, remove, 0.1, entropy, 48, 6.591, 79.93
1, 0.5, False, remove, 0.1, depth, 48, 9.320, 81.65
1, 0.5, False, remove, 0.1, entropy, 48, 12.048, 83.10
1, 0.33, False, remove, 0.1, depth, 48, 14.743, 81.11
1, 0.33, False, remove, 0.1, entropy, 48, 17.520, 82.40
1, 0.75, False, remove, 0.1, depth, 48, 20.213, 82.34
1, 0.75, False, remove, 0.1, entropy, 48, 22.966, 84.20
1, 1.0, False, remove, 0.1, depth, 48, 25.686, 82.98
1, 1.0, False, remove, 0.1, entropy, 48, 28.436, 84.79
1, 3.0, False, remove, 0.1, depth, 48, 31.281, 83.97
1, 3.0, False, remove, 0.1, entropy, 48, 34.056, 85.84
1, 5.0, False, remove, 0.1, depth, 48, 36.769, 84.58
1, 5.0, False, remove, 0.1, entropy, 48, 39.541, 86.17
3, 0.1, False, remove, 0.1, depth, 48, 5.214, 80.70
3, 0.1, False, remove, 0.1, entropy, 48, 7.986, 81.70
3, 0.5, False, remove, 0.1, depth, 48, 10.754, 82.89
3, 0.5, False, remove, 0.1, entropy, 48, 13.557, 84.05
3, 0.33, False, remove, 0.1, depth, 48, 16.286, 82.18
3, 0.33, False, remove, 0.1, entropy, 48, 19.072, 83.37
3, 0.75, False, remove, 0.1, depth, 48, 21.809, 83.51
3, 0.75, False, remove, 0.1, entropy, 48, 24.658, 85.03
3, 1.0, False, remove, 0.1, depth, 48, 27.451, 83.90
3, 1.0, False, remove, 0.1, entropy, 48, 30.224, 85.50
3, 3.0, False, remove, 0.1, depth, 48, 33.007, 85.32
3, 3.0, False, remove, 0.1, entropy, 48, 35.808, 86.60
3, 5.0, False, remove, 0.1, depth, 48, 38.560, 85.69
3, 5.0, False, remove, 0.1, entropy, 48, 41.377, 86.86
5, 0.1, False, remove, 0.1, depth, 48, 5.411, 80.44
5, 0.1, False, remove, 0.1, entropy, 48, 8.185, 81.48
5, 0.5, False, remove, 0.1, depth, 48, 10.999, 83.17
5, 0.5, False, remove, 0.1, entropy, 48, 13.774, 83.83
5, 0.33, False, remove, 0.1, depth, 48, 16.559, 82.27
5, 0.33, False, remove, 0.1, entropy, 48, 19.399, 83.53
5, 0.75, False, remove, 0.1, depth, 48, 22.181, 83.59
5, 0.75, False, remove, 0.1, entropy, 48, 25.000, 84.57
5, 1.0, False, remove, 0.1, depth, 48, 27.778, 84.14
5, 1.0, False, remove, 0.1, entropy, 48, 30.556, 85.13
5, 3.0, False, remove, 0.1, depth, 48, 33.359, 85.62
5, 3.0, False, remove, 0.1, entropy, 48, 36.184, 86.40
5, 5.0, False, remove, 0.1, depth, 48, 38.933, 86.02
5, 5.0, False, remove, 0.1, entropy, 48, 41.762, 86.73
7, 0.1, False, remove, 0.1, depth, 48, 5.455, 80.83
7, 0.1, False, remove, 0.1, entropy, 48, 8.230, 81.76
7, 0.5, False, remove, 0.1, depth, 48, 11.054, 82.46
7, 0.5, False, remove, 0.1, entropy, 48, 13.821, 84.00
7, 0.33, False, remove, 0.1, depth, 48, 16.620, 81.91
7, 0.33, False, remove, 0.1, entropy, 48, 19.446, 83.29
7, 0.75, False, remove, 0.1, depth, 48, 22.222, 83.08
7, 0.75, False, remove, 0.1, entropy, 48, 25.039, 84.64
7, 1.0, False, remove, 0.1, depth, 48, 27.854, 83.75
7, 1.0, False, remove, 0.1, entropy, 48, 30.680, 85.22
7, 3.0, False, remove, 0.1, depth, 48, 33.517, 85.34
7, 3.0, False, remove, 0.1, entropy, 48, 36.382, 86.24
7, 5.0, False, remove, 0.1, depth, 48, 39.149, 85.31
7, 5.0, False, remove, 0.1, entropy, 48, 41.978, 86.66
10, 0.1, False, remove, 0.1, depth, 48, 7.188, 80.80
10, 0.1, False, remove, 0.1, entropy, 48, 10.059, 81.63
10, 0.5, False, remove, 0.1, depth, 48, 12.923, 82.52
10, 0.5, False, remove, 0.1, entropy, 48, 15.746, 83.72
10, 0.33, False, remove, 0.1, depth, 48, 18.564, 82.16
10, 0.33, False, remove, 0.1, entropy, 48, 21.400, 83.13
10, 0.75, False, remove, 0.1, depth, 48, 24.179, 83.20
10, 0.75, False, remove, 0.1, entropy, 48, 27.035, 84.43
10, 1.0, False, remove, 0.1, depth, 48, 29.835, 83.77
10, 1.0, False, remove, 0.1, entropy, 48, 32.664, 84.92
10, 3.0, False, remove, 0.1, depth, 48, 35.519, 85.23
10, 3.0, False, remove, 0.1, entropy, 48, 38.366, 86.24
10, 5.0, False, remove, 0.1, depth, 48, 41.179, 85.62
10, 5.0, False, remove, 0.1, entropy, 48, 44.034, 86.81
1, 0.1, True, remove, 0.0, depth, 48, 3.818, 79.05
1, 0.1, True, remove, 0.0, entropy, 48, 6.575, 80.30
1, 0.5, True, remove, 0.0, depth, 48, 9.296, 81.76
1, 0.5, True, remove, 0.0, entropy, 48, 12.047, 83.22
1, 0.33, True, remove, 0.0, depth, 48, 14.775, 80.86
1, 0.33, True, remove, 0.0, entropy, 48, 17.489, 82.31
1, 0.75, True, remove, 0.0, depth, 48, 20.226, 82.33
1, 0.75, True, remove, 0.0, entropy, 48, 22.966, 84.05
1, 1.0, True, remove, 0.0, depth, 48, 25.702, 82.89
1, 1.0, True, remove, 0.0, entropy, 48, 28.433, 84.33
1, 3.0, True, remove, 0.0, depth, 48, 31.133, 84.14
1, 3.0, True, remove, 0.0, entropy, 48, 33.897, 85.87
1, 5.0, True, remove, 0.0, depth, 48, 36.614, 84.43
1, 5.0, True, remove, 0.0, entropy, 48, 39.325, 85.95
3, 0.1, True, remove, 0.0, depth, 48, 5.126, 80.21
3, 0.1, True, remove, 0.0, entropy, 48, 7.928, 81.51
3, 0.5, True, remove, 0.0, depth, 48, 10.675, 82.67
3, 0.5, True, remove, 0.0, entropy, 48, 13.511, 83.83
3, 0.33, True, remove, 0.0, depth, 48, 16.243, 82.06
3, 0.33, True, remove, 0.0, entropy, 48, 19.036, 83.05
3, 0.75, True, remove, 0.0, depth, 48, 21.790, 83.29
3, 0.75, True, remove, 0.0, entropy, 48, 24.534, 84.63
3, 1.0, True, remove, 0.0, depth, 48, 27.277, 83.74
3, 1.0, True, remove, 0.0, entropy, 48, 30.073, 85.14
3, 3.0, True, remove, 0.0, depth, 48, 32.816, 85.01
3, 3.0, True, remove, 0.0, entropy, 48, 35.606, 86.38
3, 5.0, True, remove, 0.0, depth, 48, 38.368, 85.29
3, 5.0, True, remove, 0.0, entropy, 48, 41.126, 86.48
5, 0.1, True, remove, 0.0, depth, 48, 5.338, 80.61
5, 0.1, True, remove, 0.0, entropy, 48, 8.149, 81.62
5, 0.5, True, remove, 0.0, depth, 48, 10.892, 83.11
5, 0.5, True, remove, 0.0, entropy, 48, 13.695, 84.20
5, 0.33, True, remove, 0.0, depth, 48, 16.447, 82.33
5, 0.33, True, remove, 0.0, entropy, 48, 19.266, 83.50
5, 0.75, True, remove, 0.0, depth, 48, 22.034, 83.74
5, 0.75, True, remove, 0.0, entropy, 48, 24.810, 84.74
5, 1.0, True, remove, 0.0, depth, 48, 27.619, 84.20
5, 1.0, True, remove, 0.0, entropy, 48, 30.422, 85.22
5, 3.0, True, remove, 0.0, depth, 48, 33.160, 85.13
5, 3.0, True, remove, 0.0, entropy, 48, 35.976, 86.54
5, 5.0, True, remove, 0.0, depth, 48, 38.752, 85.69
5, 5.0, True, remove, 0.0, entropy, 48, 41.529, 86.64
7, 0.1, True, remove, 0.0, depth, 48, 5.414, 81.10
7, 0.1, True, remove, 0.0, entropy, 48, 8.212, 82.21
7, 0.5, True, remove, 0.0, depth, 48, 11.008, 83.02
7, 0.5, True, remove, 0.0, entropy, 48, 13.802, 84.28
7, 0.33, True, remove, 0.0, depth, 48, 16.570, 82.46
7, 0.33, True, remove, 0.0, entropy, 48, 19.410, 83.74
7, 0.75, True, remove, 0.0, depth, 48, 22.204, 83.69
7, 0.75, True, remove, 0.0, entropy, 48, 24.980, 84.83
7, 1.0, True, remove, 0.0, depth, 48, 27.776, 84.06
7, 1.0, True, remove, 0.0, entropy, 48, 30.591, 85.43
7, 3.0, True, remove, 0.0, depth, 48, 33.372, 85.41
7, 3.0, True, remove, 0.0, entropy, 48, 36.220, 86.42
7, 5.0, True, remove, 0.0, depth, 48, 39.248, 85.57
7, 5.0, True, remove, 0.0, entropy, 48, 42.179, 86.86
10, 0.1, True, remove, 0.0, depth, 48, 7.119, 80.73
10, 0.1, True, remove, 0.0, entropy, 48, 9.929, 81.48
10, 0.5, True, remove, 0.0, depth, 48, 12.750, 82.56
10, 0.5, True, remove, 0.0, entropy, 48, 15.599, 83.91
10, 0.33, True, remove, 0.0, depth, 48, 18.405, 82.27
10, 0.33, True, remove, 0.0, entropy, 48, 21.272, 83.05
10, 0.75, True, remove, 0.0, depth, 48, 24.108, 83.25
10, 0.75, True, remove, 0.0, entropy, 48, 26.911, 84.60
10, 1.0, True, remove, 0.0, depth, 48, 29.718, 83.74
10, 1.0, True, remove, 0.0, entropy, 48, 32.553, 85.19
10, 3.0, True, remove, 0.0, depth, 48, 35.374, 85.20
10, 3.0, True, remove, 0.0, entropy, 48, 38.259, 86.35
10, 5.0, True, remove, 0.0, depth, 48, 41.054, 85.66
10, 5.0, True, remove, 0.0, entropy, 48, 43.896, 86.91
1, 0.1, True, remove, 0.25, depth, 48, 3.806, 78.80
1, 0.1, True, remove, 0.25, entropy, 48, 6.618, 79.81
1, 0.5, True, remove, 0.25, depth, 48, 9.368, 81.47
1, 0.5, True, remove, 0.25, entropy, 48, 12.123, 83.02
1, 0.33, True, remove, 0.25, depth, 48, 14.862, 80.89
1, 0.33, True, remove, 0.25, entropy, 48, 17.623, 82.24
1, 0.75, True, remove, 0.25, depth, 48, 20.360, 82.21
1, 0.75, True, remove, 0.25, entropy, 48, 23.145, 84.26
1, 1.0, True, remove, 0.25, depth, 48, 25.870, 82.70
1, 1.0, True, remove, 0.25, entropy, 48, 28.648, 84.70
1, 3.0, True, remove, 0.25, depth, 48, 31.380, 84.17
1, 3.0, True, remove, 0.25, entropy, 48, 34.091, 85.32
1, 5.0, True, remove, 0.25, depth, 48, 36.900, 84.27
1, 5.0, True, remove, 0.25, entropy, 48, 39.648, 85.34
3, 0.1, True, remove, 0.25, depth, 48, 5.243, 80.36
3, 0.1, True, remove, 0.25, entropy, 48, 8.039, 81.22
3, 0.5, True, remove, 0.25, depth, 48, 10.822, 82.79
3, 0.5, True, remove, 0.25, entropy, 48, 13.604, 84.21
3, 0.33, True, remove, 0.25, depth, 48, 16.401, 82.06
3, 0.33, True, remove, 0.25, entropy, 48, 19.197, 83.16
3, 0.75, True, remove, 0.25, depth, 48, 21.963, 83.44
3, 0.75, True, remove, 0.25, entropy, 48, 24.748, 85.07
3, 1.0, True, remove, 0.25, depth, 48, 27.509, 83.90
3, 1.0, True, remove, 0.25, entropy, 48, 30.293, 85.47
3, 3.0, True, remove, 0.25, depth, 48, 33.051, 84.68
3, 3.0, True, remove, 0.25, entropy, 48, 35.807, 86.57
3, 5.0, True, remove, 0.25, depth, 48, 38.589, 85.11
3, 5.0, True, remove, 0.25, entropy, 48, 41.384, 86.79
5, 0.1, True, remove, 0.25, depth, 48, 5.312, 81.07
5, 0.1, True, remove, 0.25, entropy, 48, 8.129, 81.73
5, 0.5, True, remove, 0.25, depth, 48, 10.918, 82.64
5, 0.5, True, remove, 0.25, entropy, 48, 13.688, 83.93
5, 0.33, True, remove, 0.25, depth, 48, 16.490, 81.97
5, 0.33, True, remove, 0.25, entropy, 48, 19.320, 82.99
5, 0.75, True, remove, 0.25, depth, 48, 22.120, 83.37
5, 0.75, True, remove, 0.25, entropy, 48, 24.951, 84.68
5, 1.0, True, remove, 0.25, depth, 48, 27.717, 83.87
5, 1.0, True, remove, 0.25, entropy, 48, 30.539, 85.31
5, 3.0, True, remove, 0.25, depth, 48, 33.328, 85.29
5, 3.0, True, remove, 0.25, entropy, 48, 36.191, 86.58
5, 5.0, True, remove, 0.25, depth, 48, 39.020, 85.57
5, 5.0, True, remove, 0.25, entropy, 48, 41.841, 86.94
7, 0.1, True, remove, 0.25, depth, 48, 5.469, 80.83
7, 0.1, True, remove, 0.25, entropy, 48, 8.313, 81.66
7, 0.5, True, remove, 0.25, depth, 48, 11.145, 83.04
7, 0.5, True, remove, 0.25, entropy, 48, 13.956, 84.34
7, 0.33, True, remove, 0.25, depth, 48, 16.757, 82.27
7, 0.33, True, remove, 0.25, entropy, 48, 19.530, 83.40
7, 0.75, True, remove, 0.25, depth, 48, 22.349, 83.74
7, 0.75, True, remove, 0.25, entropy, 48, 25.183, 84.94
7, 1.0, True, remove, 0.25, depth, 48, 27.975, 84.26
7, 1.0, True, remove, 0.25, entropy, 48, 30.792, 85.53
7, 3.0, True, remove, 0.25, depth, 48, 33.595, 85.66
7, 3.0, True, remove, 0.25, entropy, 48, 36.418, 86.89
7, 5.0, True, remove, 0.25, depth, 48, 39.233, 85.93
7, 5.0, True, remove, 0.25, entropy, 48, 42.058, 87.09
10, 0.1, True, remove, 0.25, depth, 48, 6.949, 80.92
10, 0.1, True, remove, 0.25, entropy, 48, 9.793, 81.44
10, 0.5, True, remove, 0.25, depth, 48, 12.559, 82.54
10, 0.5, True, remove, 0.25, entropy, 48, 15.389, 83.91
10, 0.33, True, remove, 0.25, depth, 48, 18.195, 81.93
10, 0.33, True, remove, 0.25, entropy, 48, 21.004, 83.31
10, 0.75, True, remove, 0.25, depth, 48, 23.816, 83.22
10, 0.75, True, remove, 0.25, entropy, 48, 26.642, 84.63
10, 1.0, True, remove, 0.25, depth, 48, 29.432, 83.74
10, 1.0, True, remove, 0.25, entropy, 48, 32.295, 85.28
10, 3.0, True, remove, 0.25, depth, 48, 35.132, 85.35
10, 3.0, True, remove, 0.25, entropy, 48, 37.929, 86.63
10, 5.0, True, remove, 0.25, depth, 48, 40.751, 85.84
10, 5.0, True, remove, 0.25, entropy, 48, 43.608, 87.09
1, 0.1, True, remove, 0.1, depth, 48, 3.819, 78.83
1, 0.1, True, remove, 0.1, entropy, 48, 6.539, 79.78
1, 0.5, True, remove, 0.1, depth, 48, 9.274, 81.51
1, 0.5, True, remove, 0.1, entropy, 48, 12.018, 82.64
1, 0.33, True, remove, 0.1, depth, 48, 14.725, 80.58
1, 0.33, True, remove, 0.1, entropy, 48, 17.497, 81.81
1, 0.75, True, remove, 0.1, depth, 48, 20.259, 82.12
1, 0.75, True, remove, 0.1, entropy, 48, 22.974, 83.65
1, 1.0, True, remove, 0.1, depth, 48, 25.713, 82.67
1, 1.0, True, remove, 0.1, entropy, 48, 28.428, 84.15
1, 3.0, True, remove, 0.1, depth, 48, 31.157, 83.96
1, 3.0, True, remove, 0.1, entropy, 48, 33.901, 85.46
1, 5.0, True, remove, 0.1, depth, 48, 36.605, 84.21
1, 5.0, True, remove, 0.1, entropy, 48, 39.367, 85.83
3, 0.1, True, remove, 0.1, depth, 48, 5.214, 81.13
3, 0.1, True, remove, 0.1, entropy, 48, 7.979, 81.96
3, 0.5, True, remove, 0.1, depth, 48, 10.775, 83.60
3, 0.5, True, remove, 0.1, entropy, 48, 13.563, 84.94
3, 0.33, True, remove, 0.1, depth, 48, 16.343, 82.91
3, 0.33, True, remove, 0.1, entropy, 48, 19.128, 84.17
3, 0.75, True, remove, 0.1, depth, 48, 21.919, 84.26
3, 0.75, True, remove, 0.1, entropy, 48, 24.692, 85.52
3, 1.0, True, remove, 0.1, depth, 48, 27.507, 84.51
3, 1.0, True, remove, 0.1, entropy, 48, 30.253, 85.93
3, 3.0, True, remove, 0.1, depth, 48, 33.030, 85.81
3, 3.0, True, remove, 0.1, entropy, 48, 35.837, 86.91
3, 5.0, True, remove, 0.1, depth, 48, 38.589, 85.92
3, 5.0, True, remove, 0.1, entropy, 48, 41.405, 87.16
5, 0.1, True, remove, 0.1, depth, 48, 5.371, 80.93
5, 0.1, True, remove, 0.1, entropy, 48, 8.151, 81.68
5, 0.5, True, remove, 0.1, depth, 48, 10.949, 83.16
5, 0.5, True, remove, 0.1, entropy, 48, 13.732, 84.45
5, 0.33, True, remove, 0.1, depth, 48, 16.493, 82.34
5, 0.33, True, remove, 0.1, entropy, 48, 19.292, 83.40
5, 0.75, True, remove, 0.1, depth, 48, 22.048, 83.71
5, 0.75, True, remove, 0.1, entropy, 48, 24.866, 85.16
5, 1.0, True, remove, 0.1, depth, 48, 27.668, 84.21
5, 1.0, True, remove, 0.1, entropy, 48, 30.478, 85.63
5, 3.0, True, remove, 0.1, depth, 48, 33.274, 85.25
5, 3.0, True, remove, 0.1, entropy, 48, 36.093, 86.57
5, 5.0, True, remove, 0.1, depth, 48, 38.842, 85.59
5, 5.0, True, remove, 0.1, entropy, 48, 41.655, 86.86
7, 0.1, True, remove, 0.1, depth, 48, 5.435, 80.76
7, 0.1, True, remove, 0.1, entropy, 48, 8.232, 81.45
7, 0.5, True, remove, 0.1, depth, 48, 11.056, 82.71
7, 0.5, True, remove, 0.1, entropy, 48, 14.094, 83.97
7, 0.33, True, remove, 0.1, depth, 48, 16.918, 82.25
7, 0.33, True, remove, 0.1, entropy, 48, 19.794, 83.05
7, 0.75, True, remove, 0.1, depth, 48, 22.572, 83.32
7, 0.75, True, remove, 0.1, entropy, 48, 25.409, 84.67
7, 1.0, True, remove, 0.1, depth, 48, 28.223, 83.65
7, 1.0, True, remove, 0.1, entropy, 48, 31.005, 85.19
7, 3.0, True, remove, 0.1, depth, 48, 33.841, 84.91
7, 3.0, True, remove, 0.1, entropy, 48, 36.708, 86.67
7, 5.0, True, remove, 0.1, depth, 48, 39.492, 85.50
7, 5.0, True, remove, 0.1, entropy, 48, 42.341, 86.91
10, 0.1, True, remove, 0.1, depth, 48, 7.058, 80.98
10, 0.1, True, remove, 0.1, entropy, 48, 9.835, 81.73
10, 0.5, True, remove, 0.1, depth, 48, 12.665, 82.95
10, 0.5, True, remove, 0.1, entropy, 48, 15.473, 84.03
10, 0.33, True, remove, 0.1, depth, 48, 18.294, 82.45
10, 0.33, True, remove, 0.1, entropy, 48, 21.127, 83.41
10, 0.75, True, remove, 0.1, depth, 48, 23.908, 83.42
10, 0.75, True, remove, 0.1, entropy, 48, 26.725, 84.43
10, 1.0, True, remove, 0.1, depth, 48, 29.547, 83.83
10, 1.0, True, remove, 0.1, entropy, 48, 32.362, 85.01
10, 3.0, True, remove, 0.1, depth, 48, 35.162, 84.88
10, 3.0, True, remove, 0.1, entropy, 48, 37.993, 86.32
10, 5.0, True, remove, 0.1, depth, 48, 40.772, 85.19
10, 5.0, True, remove, 0.1, entropy, 48, 43.610, 86.57
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': True, 'GAMMA': 5.0, 'NB_TRAIN_ITERATIONS': 3, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.1, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 48, 'TRAINING_TIME': 41.40531219635159, 'VALIDATION ACCURACY': 0.8716085989621942}
-----TESTING
Final accuracy with best hyperparameters: 85.81
Mem in MB: 22.45
Mem in MB: 21.18
-----TIME PROFILING+
Read train + val data time:  0.23092
Number of training symbols: 5449152
Length of one training sequence: 101
Total training time: 1378.090 seconds
Number of test sequences: 6745
Length of test sequence: 101
Read test data time:  0.01566
Total inference time: 2.816 seconds
Inference time/symbol: 4.133248051927877e-06 seconds
-----MEMORY REPORT
Filename: /data/home/nsagan/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   229    137.9 MiB    137.9 MiB           1   @profile
   230                                         def main(dataset_folder, pretrain_file):
   231                                             global INCLUDE_PREV_CONTEXT
   232                                             global GAMMA
   233                                             global NB_TRAIN_ITERATIONS 
   234                                             global HANDLE_N_SETTING 
   235                                             global RATIO_PRETRAIN_TRAIN 
   236                                             global ENSEMBLE_TYPE 
   237                                             global NUM_THREADS
   238                                             
   239                                             global include_prev_contexts
   240                                             global gammas 
   241                                             global nb_train_iterations 
   242                                             global handle_N_settings 
   243                                             global ratio_pretrain_train
   244                                             global ensemble_type
   245                                             global num_threads
   246                                         
   247    137.9 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   248                                             
   249                                             # Read train, val, test data 
   250    137.9 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   251    137.9 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   252    137.9 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   253                                             
   254                                         
   255    149.0 MiB     11.1 MiB           1       train_data = pd.read_csv(train_path)
   256    150.3 MiB      1.2 MiB           1       validation_data = pd.read_csv(val_path)
   257                                             
   258    150.3 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   259    150.7 MiB      0.4 MiB           1       unique_labels = train_data['label'].unique()
   260                                             
   261    246.1 MiB      0.0 MiB           2       with open(pretrain_file, 'r') as file:
   262    246.1 MiB     95.5 MiB           1           pretrain_data = file.read()
   263                                             
   264                                             # Train all SPAs using all possible combinations of hyperparams
   265                                             # Test all on validation set, return best SPA
   266    246.1 MiB      0.0 MiB           1       results_df = pd.DataFrame(columns=[
   267                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   268                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   269                                             ])
   270                                         
   271    246.1 MiB      0.0 MiB           1       print("-----TRAINING")
   272    246.1 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   273    246.1 MiB      0.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   274    246.1 MiB      0.0 MiB           1       train_start_time = time.perf_counter()
   275    613.2 MiB    -16.8 MiB           8       for include_prev_context, handle_N_setting, ratio in itertools.product(
   276    246.1 MiB      0.0 MiB           1           include_prev_contexts, handle_N_settings, ratio_pretrain_train
   277                                             ):  
   278    613.2 MiB    -16.4 MiB           6           INCLUDE_PREV_CONTEXT = include_prev_context
   279    613.2 MiB    -16.4 MiB           6           GAMMA = gammas
   280    613.2 MiB    -16.4 MiB           6           NB_TRAIN_ITERATIONS = 0
   281    613.2 MiB    -16.4 MiB           6           HANDLE_N_SETTING = handle_N_setting
   282    613.2 MiB    -16.4 MiB           6           RATIO_PRETRAIN_TRAIN = ratio 
   283    613.2 MiB    -16.4 MiB           6           ENSEMBLE_TYPE = ensemble_type
   284    613.2 MiB    -16.4 MiB           6           NUM_THREADS = num_threads
   285                                                 
   286    613.3 MiB      4.4 MiB           6           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   287    613.3 MiB    -16.5 MiB           6           nb_train_seqs = len(train_data)
   288    613.3 MiB    -16.5 MiB           6           seq_len = len(train_data.iloc[0, 0])
   289    613.3 MiB    -16.5 MiB           6           nb_train_symbols = nb_train_seqs * seq_len
   290                                                 
   291                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   292    613.3 MiB  -1202.5 MiB          30           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, compute_training_loss=False) for _ in unique_labels]
   293    395.4 MiB  -1136.4 MiB          18           for i in range(len(unique_labels)):
   294    395.4 MiB      0.0 MiB          24               spa[i].set_inference_config(
   295    395.4 MiB      0.0 MiB          12                   lb=1e-5,
   296    395.4 MiB      0.0 MiB          12                   ensemble_type="entropy",
   297    395.4 MiB      0.0 MiB          12                   ensemble_n=10,
   298    395.4 MiB      0.0 MiB          12                   backshift_parsing=True,
   299    395.4 MiB      0.0 MiB          12                   backshift_ctx_len=20,
   300    395.4 MiB      0.0 MiB          12                   backshift_break_at_phrase=True
   301                                                     )
   302                                         
   303    395.4 MiB      0.0 MiB           6           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   304    395.4 MiB    108.8 MiB           6           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   305                                         
   306    395.4 MiB      0.0 MiB           6           iterated_times = 0
   307    613.2 MiB  -3098.7 MiB          36           for nb_iterations in nb_train_iterations:
   308    545.4 MiB  -4097.7 MiB          30               train_one_iter_start_time = time.perf_counter()
   309    613.3 MiB  -7106.4 MiB          90               for _ in range(nb_iterations - iterated_times):
   310    613.3 MiB  -5182.0 MiB          60                   train_spa_oneIter(train_data, spa)
   311                                                     
   312    613.3 MiB  -3095.3 MiB          30               iterated_times = nb_iterations
   313    613.4 MiB -24787.0 MiB         240               for gamma in gammas:
   314    613.4 MiB -65077.1 MiB         630                   for ensemble in ENSEMBLE_TYPE:
   315                                                         # Test on validation test to assess this combination of hyperparams
   316    613.4 MiB -43373.8 MiB         420                       validation_data = handle_N(validation_data)
   317    613.4 MiB -130126.7 MiB        1260                       for index in range(len(spa)):
   318    613.4 MiB -86751.1 MiB         840                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   319    613.4 MiB -43377.3 MiB         420                       accuracy = test_seq(validation_data, spa, num_threads)
   320    613.4 MiB -43387.0 MiB         420                       train_one_iter_end_time = time.perf_counter()
   321    613.4 MiB -43387.0 MiB         420                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   322    613.4 MiB -43387.0 MiB         420                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   323                                         
   324                                                         
   325                                                         
   326    613.4 MiB -86773.9 MiB         840                       current_result = pd.DataFrame([{
   327    613.4 MiB -43387.0 MiB         420                           "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   328    613.4 MiB -43387.0 MiB         420                           "GAMMA": gamma,
   329    613.4 MiB -43387.0 MiB         420                           "NB_TRAIN_ITERATIONS": nb_iterations,
   330    613.4 MiB -43387.0 MiB         420                           "HANDLE_N_SETTING": HANDLE_N_SETTING,
   331    613.4 MiB -43387.0 MiB         420                           "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   332    613.4 MiB -43387.0 MiB         420                           "ENSEMBLE_TYPE": ensemble,
   333    613.4 MiB -43387.0 MiB         420                           "NUM_THREADS": NUM_THREADS,
   334    613.4 MiB -43387.0 MiB         420                           "TRAINING_TIME": train_one_iter_duration, 
   335    613.4 MiB -43387.0 MiB         420                           "VALIDATION ACCURACY": accuracy
   336                                                             }])
   337                                         
   338                                                         # Concatenate the current result with results_df
   339    613.4 MiB -21692.9 MiB         210                   results_df = results_df.dropna(axis=1, how='all')
   340    613.4 MiB -21691.7 MiB         210                   current_result = current_result.dropna(axis=1, how='all')
   341                                         
   342    613.4 MiB -21691.5 MiB         210                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   343                                         
   344                                             
   345                                             # Find the best hyperparameter combination based on the highest accuracy
   346    612.8 MiB     -0.4 MiB           1       print("---BEST SPA(s) FOUND")
   347    612.8 MiB      0.0 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   348    612.8 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   349    612.8 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   350                                         
   351                                             # Retrain and test using the best hyperparameters
   352    612.8 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   353    612.8 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   354    612.8 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   355    612.8 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   356    612.8 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   357    612.8 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   358    612.8 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   359                                         
   360                                             # Retrain our best SPAs and use that to test on test data 
   361    612.8 MiB   -217.4 MiB           5       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   362    395.4 MiB   -217.4 MiB           3       for i in range(len(unique_labels)):
   363    395.4 MiB      0.0 MiB           4           spa[i].set_inference_config(
   364    395.4 MiB      0.0 MiB           2               lb=1e-5,
   365    395.4 MiB      0.0 MiB           2               ensemble_type= ENSEMBLE_TYPE,
   366    395.4 MiB      0.0 MiB           2               ensemble_n=10,
   367    395.4 MiB      0.0 MiB           2               backshift_parsing=True,
   368    395.4 MiB      0.0 MiB           2               backshift_ctx_len=20,
   369    395.4 MiB      0.0 MiB           2               backshift_break_at_phrase=True
   370                                                 )
   371                                         
   372    395.4 MiB      0.0 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   373    395.4 MiB      0.0 MiB           1       nb_train_seqs = len(train_data)
   374    395.4 MiB      0.0 MiB           1       seq_len = len(train_data.iloc[0, 0])
   375    395.4 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   376    395.4 MiB      0.0 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   377                                         
   378    395.4 MiB      0.0 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   379    395.4 MiB      0.0 MiB           1       train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   380                                         
   381    395.4 MiB      0.0 MiB           1       train_end_time = time.perf_counter()
   382    395.4 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   383                                         
   384                                             
   385                                             
   386                                             # Final test
   387    395.4 MiB      0.0 MiB           1       print("-----TESTING")
   388    395.4 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   389    395.4 MiB      0.0 MiB           1       test_data = pd.read_csv(test_path)
   390                                         
   391    395.4 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   392                                         
   393    395.4 MiB      0.0 MiB           1       test_data = handle_N(test_data)
   394    395.3 MiB     -0.1 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   395                                         
   396    395.3 MiB      0.0 MiB           1       inference_end_time = time.perf_counter()
   397    395.3 MiB      0.0 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   398                                             
   399                                                 
   400    395.3 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   401                                         
   402    395.3 MiB      0.0 MiB           1       label = 0
   403    449.2 MiB      0.0 MiB           3       for sp in spa:
   404    449.2 MiB     53.9 MiB           2           spa_bytes = bytearray(sp.to_bytes())
   405    449.2 MiB      0.0 MiB           2           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   406    449.2 MiB      0.0 MiB           2           makedirs("best_spas", exist_ok=True)
   407                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   408    449.2 MiB      0.0 MiB           2           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   409                                                 
   410                                                 # Create the full path for the binary file
   411    449.2 MiB      0.0 MiB           2           binary_file_path = os.path.join("best_spas", f"{binary_file_name}_{label}.bin")
   412    449.2 MiB      0.0 MiB           2           label += 1
   413                                                 # Save the binary file
   414    449.2 MiB      0.0 MiB           4           with open(binary_file_path, 'wb') as file:
   415    449.2 MiB      0.0 MiB           2               file.write(spa_bytes)
   416                                             
   417                                         
   418    449.2 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   419    449.2 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   420    449.2 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   421    449.2 MiB      0.0 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   422    449.2 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   423                                             
   424                                         
   425    449.2 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   426    449.2 MiB      0.0 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   427    449.2 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   428    449.2 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   429    449.2 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   430                                         
   431    449.2 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


