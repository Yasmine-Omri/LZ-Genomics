-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, True, remove, 0.0, entropy, 1, 21.824, 67.05
1, 0.1, True, remove, 0.0, depth, 1, 37.345, 66.39
1, 0.5, True, remove, 0.0, entropy, 1, 48.844, 68.03
1, 0.5, True, remove, 0.0, depth, 1, 56.679, 67.05
1, 0.33, True, remove, 0.0, entropy, 1, 67.173, 68.03
1, 0.33, True, remove, 0.0, depth, 1, 75.525, 66.72
1, 0.75, True, remove, 0.0, entropy, 1, 83.404, 67.37
1, 0.75, True, remove, 0.0, depth, 1, 89.801, 67.70
1, 1.0, True, remove, 0.0, entropy, 1, 98.253, 68.35
1, 1.0, True, remove, 0.0, depth, 1, 104.843, 67.70
1, 3.0, True, remove, 0.0, entropy, 1, 111.635, 71.94
1, 3.0, True, remove, 0.0, depth, 1, 117.808, 69.49
1, 5.0, True, remove, 0.0, entropy, 1, 124.390, 73.08
1, 5.0, True, remove, 0.0, depth, 1, 130.676, 71.13
3, 0.1, True, remove, 0.0, entropy, 1, 8.988, 70.47
3, 0.1, True, remove, 0.0, depth, 1, 16.170, 68.68
3, 0.5, True, remove, 0.0, entropy, 1, 24.122, 72.10
3, 0.5, True, remove, 0.0, depth, 1, 31.149, 70.80
3, 0.33, True, remove, 0.0, entropy, 1, 38.924, 71.94
3, 0.33, True, remove, 0.0, depth, 1, 46.362, 69.49
3, 0.75, True, remove, 0.0, entropy, 1, 54.403, 72.92
3, 0.75, True, remove, 0.0, depth, 1, 62.005, 70.80
3, 1.0, True, remove, 0.0, entropy, 1, 70.672, 72.92
3, 1.0, True, remove, 0.0, depth, 1, 83.407, 70.64
3, 3.0, True, remove, 0.0, entropy, 1, 103.282, 74.88
3, 3.0, True, remove, 0.0, depth, 1, 125.406, 72.27
3, 5.0, True, remove, 0.0, entropy, 1, 144.898, 74.88
3, 5.0, True, remove, 0.0, depth, 1, 160.468, 72.92
5, 0.1, True, remove, 0.0, entropy, 1, 16.622, 67.86
5, 0.1, True, remove, 0.0, depth, 1, 28.535, 67.05
5, 0.5, True, remove, 0.0, entropy, 1, 39.906, 69.49
5, 0.5, True, remove, 0.0, depth, 1, 55.570, 68.84
5, 0.33, True, remove, 0.0, entropy, 1, 69.299, 69.17
5, 0.33, True, remove, 0.0, depth, 1, 82.449, 68.84
5, 0.75, True, remove, 0.0, entropy, 1, 94.842, 70.96
5, 0.75, True, remove, 0.0, depth, 1, 105.760, 68.84
5, 1.0, True, remove, 0.0, entropy, 1, 117.732, 71.94
5, 1.0, True, remove, 0.0, depth, 1, 126.607, 69.00
5, 3.0, True, remove, 0.0, entropy, 1, 140.069, 72.92
5, 3.0, True, remove, 0.0, depth, 1, 151.791, 71.45
5, 5.0, True, remove, 0.0, entropy, 1, 168.665, 73.90
5, 5.0, True, remove, 0.0, depth, 1, 183.840, 72.76
7, 0.1, True, remove, 0.0, entropy, 1, 11.776, 68.84
7, 0.1, True, remove, 0.0, depth, 1, 21.550, 67.05
7, 0.5, True, remove, 0.0, entropy, 1, 31.434, 70.80
7, 0.5, True, remove, 0.0, depth, 1, 40.047, 68.52
7, 0.33, True, remove, 0.0, entropy, 1, 49.334, 70.47
7, 0.33, True, remove, 0.0, depth, 1, 57.837, 68.19
7, 0.75, True, remove, 0.0, entropy, 1, 67.172, 71.62
7, 0.75, True, remove, 0.0, depth, 1, 76.335, 69.49
7, 1.0, True, remove, 0.0, entropy, 1, 86.871, 71.62
7, 1.0, True, remove, 0.0, depth, 1, 95.930, 70.15
7, 3.0, True, remove, 0.0, entropy, 1, 105.853, 73.25
7, 3.0, True, remove, 0.0, depth, 1, 114.702, 72.76
7, 5.0, True, remove, 0.0, entropy, 1, 124.199, 74.23
7, 5.0, True, remove, 0.0, depth, 1, 133.283, 73.74
10, 0.1, True, remove, 0.0, entropy, 1, 14.257, 69.82
10, 0.1, True, remove, 0.0, depth, 1, 24.241, 69.33
10, 0.5, True, remove, 0.0, entropy, 1, 34.738, 72.10
10, 0.5, True, remove, 0.0, depth, 1, 45.242, 69.66
10, 0.33, True, remove, 0.0, entropy, 1, 56.571, 70.96
10, 0.33, True, remove, 0.0, depth, 1, 67.343, 69.66
10, 0.75, True, remove, 0.0, entropy, 1, 78.007, 71.78
10, 0.75, True, remove, 0.0, depth, 1, 88.312, 70.31
10, 1.0, True, remove, 0.0, entropy, 1, 99.677, 72.10
10, 1.0, True, remove, 0.0, depth, 1, 108.628, 71.13
10, 3.0, True, remove, 0.0, entropy, 1, 119.244, 73.57
10, 3.0, True, remove, 0.0, depth, 1, 128.661, 72.27
10, 5.0, True, remove, 0.0, entropy, 1, 138.945, 74.39
10, 5.0, True, remove, 0.0, depth, 1, 148.544, 73.90
entropy, 1, 139.401, 73.25
4, 1.5, True, remove, 0.0, depth, 1, 154.504, 72.10
4, 2.0, True, remove, 0.0, entropy, 1, 164.082, 73.08
4, 2.0, True, remove, 0.0, depth, 1, 172.906, 72.92
4, 2.5, True, remove, 0.0, entropy, 1, 182.691, 73.74
4, 2.5, True, remove, 0.0, depth, 1, 190.639, 73.41
4, 3.0, True, remove, 0.0, entropy, 1, 199.115, 73.25
4, 3.0, True, remove, 0.0, depth, 1, 206.986, 73.90
4, 5.0, True, remove, 0.0, entropy, 1, 215.329, 73.57
4, 5.0, True, remove, 0.0, depth, 1, 223.219, 75.86
5, 0.1, True, remove, 0.0, entropy, 1, 9.886, 67.86
5, 0.1, True, remove, 0.0, depth, 1, 18.333, 67.05
5, 0.5, True, remove, 0.0, entropy, 1, 27.480, 69.49
5, 0.5, True, remove, 0.0, depth, 1, 35.850, 68.84
5, 0.33, True, remove, 0.0, entropy, 1, 44.681, 69.17
5, 0.33, True, remove, 0.0, depth, 1, 52.926, 68.84
5, 0.75, True, remove, 0.0, entropy, 1, 62.215, 70.96
5, 0.75, True, remove, 0.0, depth, 1, 71.174, 68.84
5, 1.0, True, remove, 0.0, entropy, 1, 82.193, 71.94
5, 1.0, True, remove, 0.0, depth, 1, 91.710, 69.00
5, 1.5, True, remove, 0.0, entropy, 1, 102.023, 72.43
5, 1.5, True, remove, 0.0, depth, 1, 111.306, 70.80
5, 2.0, True, remove, 0.0, entropy, 1, 121.396, 73.57
5, 2.0, True, remove, 0.0, depth, 1, 130.619, 70.80
5, 2.5, True, remove, 0.0, entropy, 1, 140.387, 72.92
5, 2.5, True, remove, 0.0, depth, 1, 149.649, 71.45
5, 3.0, True, remove, 0.0, entropy, 1, 159.736, 72.92
5, 3.0, True, remove, 0.0, depth, 1, 169.954, 71.45
5, 5.0, True, remove, 0.0, entropy, 1, 179.260, 73.90
5, 5.0, True, remove, 0.0, depth, 1, 187.851, 72.76
6, 0.1, True, remove, 0.0, entropy, 1, 11.008, 68.19
6, 0.1, True, remove, 0.0, depth, 1, 20.559, 67.70
6, 0.5, True, remove, 0.0, entropy, 1, 30.721, 70.15
6, 0.5, True, remove, 0.0, depth, 1, 43.431, 68.52
6, 0.33, True, remove, 0.0, entropy, 1, 62.414, 69.49
6, 0.33, True, remove, 0.0, depth, 1, 78.257, 67.70
6, 0.75, True, remove, 0.0, entropy, 1, 92.124, 71.13
6, 0.75, True, remove, 0.0, depth, 1, 101.837, 69.33
6, 1.0, True, remove, 0.0, entropy, 1, 111.523, 71.94
6, 1.0, True, remove, 0.0, depth, 1, 120.684, 70.64
6, 1.5, True, remove, 0.0, entropy, 1, 130.529, 71.78
6, 1.5, True, remove, 0.0, depth, 1, 140.800, 71.29
6, 2.0, True, remove, 0.0, entropy, 1, 148.569, 72.43
6, 2.0, True, remove, 0.0, depth, 1, 156.702, 71.94
6, 2.5, True, remove, 0.0, entropy, 1, 166.672, 72.76
6, 2.5, True, remove, 0.0, depth, 1, 176.106, 72.59
6, 3.0, True, remove, 0.0, entropy, 1, 186.070, 73.08
6, 3.0, True, remove, 0.0, depth, 1, 195.390, 72.92
6, 5.0, True, remove, 0.0, entropy, 1, 204.684, 73.25
6, 5.0, True, remove, 0.0, depth, 1, 214.583, 73.41
7, 0.1, True, remove, 0.0, entropy, 1, 11.387, 68.84
7, 0.1, True, remove, 0.0, depth, 1, 22.279, 67.05
7, 0.5, True, remove, 0.0, entropy, 1, 33.169, 70.80
7, 0.5, True, remove, 0.0, depth, 1, 43.259, 68.52
7, 0.33, True, remove, 0.0, entropy, 1, 54.838, 70.47
7, 0.33, True, remove, 0.0, depth, 1, 64.405, 68.19
7, 0.75, True, remove, 0.0, entropy, 1, 74.257, 71.62
7, 0.75, True, remove, 0.0, depth, 1, 84.050, 69.49
7, 1.0, True, remove, 0.0, entropy, 1, 95.081, 71.62
7, 1.0, True, remove, 0.0, depth, 1, 105.914, 70.15
7, 1.5, True, remove, 0.0, entropy, 1, 117.573, 70.96
7, 1.5, True, remove, 0.0, depth, 1, 127.962, 69.66
7, 2.0, True, remove, 0.0, entropy, 1, 138.724, 71.94
7, 2.0, True, remove, 0.0, depth, 1, 149.887, 70.80
7, 2.5, True, remove, 0.0, entropy, 1, 161.738, 73.57
7, 2.5, True, remove, 0.0, depth, 1, 171.420, 72.10
7, 3.0, True, remove, 0.0, entropy, 1, 183.659, 73.25
7, 3.0, True, remove, 0.0, depth, 1, 200.174, 72.76
7, 5.0, True, remove, 0.0, entropy, 1, 309.543, 74.23
7, 5.0, True, remove, 0.0, depth, 1, 327.185, 73.74
8, 0.1, True, remove, 0.0, entropy, 1, 12.928, 68.68
8, 0.1, True, remove, 0.0, depth, 1, 28.827, 68.68
8, 0.5, True, remove, 0.0, entropy, 1, 43.320, 71.13
8, 0.5, True, remove, 0.0, depth, 1, 55.731, 69.49
8, 0.33, True, remove, 0.0, entropy, 1, 68.429, 70.80
8, 0.33, True, remove, 0.0, depth, 1, 80.145, 69.66
8, 0.75, True, remove, 0.0, entropy, 1, 92.789, 72.27
8, 0.75, True, remove, 0.0, depth, 1, 104.435, 70.15
8, 1.0, True, remove, 0.0, entropy, 1, 116.890, 72.43
8, 1.0, True, remove, 0.0, depth, 1, 128.024, 70.47
8, 1.5, True, remove, 0.0, entropy, 1, 138.995, 72.92
8, 1.5, True, remove, 0.0, depth, 1, 149.770, 71.13
8, 2.0, True, remove, 0.0, entropy, 1, 161.956, 73.41
8, 2.0, True, remove, 0.0, depth, 1, 172.699, 72.27
8, 2.5, True, remove, 0.0, entropy, 1, 185.483, 73.57
8, 2.5, True, remove, 0.0, depth, 1, 197.808, 72.59
8, 3.0, True, remove, 0.0, entropy, 1, 209.736, 73.90
8, 3.0, True, remove, 0.0, depth, 1, 220.235, 72.27
8, 5.0, True, remove, 0.0, entropy, 1, 230.341, 74.06
8, 5.0, True, remove, 0.0, depth, 1, 242.662, 73.25
9, 0.1, True, remove, 0.0, entropy, 1, 18.383, 69.17
9, 0.1, True, remove, 0.0, depth, 1, 32.920, 68.35
9, 0.5, True, remove, 0.0, entropy, 1, 47.567, 72.76
9, 0.5, True, remove, 0.0, depth, 1, 59.946, 69.66
9, 0.33, True, remove, 0.0, entropy, 1, 76.054, 70.96
9, 0.33, True, remove, 0.0, depth, 1, 89.222, 69.82
9, 0.75, True, remove, 0.0, entropy, 1, 108.008, 72.43
9, 0.75, True, remove, 0.0, depth, 1, 121.018, 70.47
9, 1.0, True, remove, 0.0, entropy, 1, 136.353, 72.27
9, 1.0, True, remove, 0.0, depth, 1, 149.482, 71.13
9, 1.5, True, remove, 0.0, entropy, 1, 163.324, 72.27
9, 1.5, True, remove, 0.0, depth, 1, 176.953, 71.78
9, 2.0, True, remove, 0.0, entropy, 1, 190.990, 72.59
9, 2.0, True, remove, 0.0, depth, 1, 203.391, 72.59
9, 2.5, True, remove, 0.0, entropy, 1, 216.776, 73.08
9, 2.5, True, remove, 0.0, depth, 1, 229.083, 72.59
9, 3.0, True, remove, 0.0, entropy, 1, 249.282, 73.90
9, 3.0, True, remove, 0.0, depth, 1, 266.038, 72.76
9, 5.0, True, remove, 0.0, entropy, 1, 280.933, 74.23
9, 5.0, True, remove, 0.0, depth, 1, 294.603, 73.74
10, 0.1, True, remove, 0.0, entropy, 1, 14.905, 69.82
10, 0.1, True, remove, 0.0, depth, 1, 28.228, 69.33
10, 0.5, True, remove, 0.0, entropy, 1, 40.192, 72.10
10, 0.5, True, remove, 0.0, depth, 1, 52.929, 69.66
10, 0.33, True, remove, 0.0, entropy, 1, 67.091, 70.96
10, 0.33, True, remove, 0.0, depth, 1, 81.208, 69.66
10, 0.75, True, remove, 0.0, entropy, 1, 95.890, 71.78
10, 0.75, True, remove, 0.0, depth, 1, 110.391, 70.31
10, 1.0, True, remove, 0.0, entropy, 1, 131.836, 72.10
10, 1.0, True, remove, 0.0, depth, 1, 145.283, 71.13
10, 1.5, True, remove, 0.0, entropy, 1, 163.561, 72.10
10, 1.5, True, remove, 0.0, depth, 1, 178.433, 71.94
10, 2.0, True, remove, 0.0, entropy, 1, 194.389, 72.59
10, 2.0, True, remove, 0.0, depth, 1, 207.897, 72.27
10, 2.5, True, remove, 0.0, entropy, 1, 220.698, 73.25
10, 2.5, True, remove, 0.0, depth, 1, 233.166, 71.62
10, 3.0, True, remove, 0.0, entropy, 1, 250.194, 73.57
10, 3.0, True, remove, 0.0, depth, 1, 268.319, 72.27
10, 5.0, True, remove, 0.0, entropy, 1, 282.907, 74.39
10, 5.0, True, remove, 0.0, depth, 1, 297.071, 73.90
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': True, 'GAMMA': 5.0, 'NB_TRAIN_ITERATIONS': 4, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.0, 'ENSEMBLE_TYPE': 'depth', 'NUM_THREADS': 1, 'TRAINING_TIME': 223.2188777080155, 'VALIDATION ACCURACY': 0.7585644371941273}
-----TESTING
Final accuracy with best hyperparameters: 73.74
Mem in MB: 2.22
Mem in MB: 2.20
-----TIME PROFILING+
Read train + val data time:  8.24516
Number of training symbols: 343280
Length of one training sequence: 70
Total training time: 2337.410 seconds
Number of test sequences: 613
Length of test sequence: 70
Read test data time:  0.34452
Total inference time: 11.050 seconds
Inference time/symbol: 0.0002575207808905187 seconds
-----MEMORY REPORT
Filename: /Users/eugenemin/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   223     32.7 MiB     32.7 MiB           1   @profile
   224                                         def main(dataset_folder, pretrain_file):
   225                                             global INCLUDE_PREV_CONTEXT
   226                                             global GAMMA
   227                                             global NB_TRAIN_ITERATIONS 
   228                                             global HANDLE_N_SETTING 
   229                                             global RATIO_PRETRAIN_TRAIN 
   230                                             global ENSEMBLE_TYPE 
   231                                             global NUM_THREADS
   232                                             
   233                                             global include_prev_contexts
   234                                             global gammas 
   235                                             global nb_train_iterations 
   236                                             global handle_N_settings 
   237                                             global ratio_pretrain_train
   238                                             global ensemble_type
   239                                             global num_threads
   240                                         
   241     32.7 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   242                                             
   243                                             # Read train, val, test data 
   244     32.7 MiB     -0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   245     32.7 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   246     32.7 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   247                                             
   248                                         
   249     28.5 MiB     -4.3 MiB           1       train_data = pd.read_csv(train_path)
   250     29.9 MiB      1.4 MiB           1       validation_data = pd.read_csv(val_path)
   251                                             
   252     29.9 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   253     31.9 MiB      2.0 MiB           1       unique_labels = train_data['label'].unique()
   254                                             
   255     31.9 MiB      1.2 MiB           2       with open(pretrain_file, 'r') as file:
   256      2.9 MiB    -29.0 MiB           1           pretrain_data = file.read()
   257                                             
   258                                             # Train all SPAs using all possible combinations of hyperparams
   259                                             # Test all on validation set, return best SPA
   260     11.9 MiB    -20.0 MiB           1       results_df = pd.DataFrame(columns=[
   261                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   262                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   263                                             ])
   264                                         
   265     12.2 MiB      0.3 MiB           1       print("-----TRAINING")
   266     12.3 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   267      4.2 MiB     -8.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   268      4.5 MiB      0.3 MiB           1       train_start_time = time.perf_counter()
   269     23.8 MiB    -50.7 MiB           3       for include_prev_context, handle_N_setting, ratio in itertools.product(
   270      4.6 MiB      0.0 MiB           1       include_prev_contexts, handle_N_settings, ratio_pretrain_train
   271                                             ):  
   272      4.8 MiB      0.0 MiB           1           INCLUDE_PREV_CONTEXT = include_prev_context
   273      4.8 MiB      0.0 MiB           1           GAMMA = gammas
   274      4.8 MiB      0.0 MiB           1           NB_TRAIN_ITERATIONS = 0
   275      4.8 MiB      0.0 MiB           1           HANDLE_N_SETTING = handle_N_setting
   276      4.8 MiB      0.1 MiB           1           RATIO_PRETRAIN_TRAIN = ratio 
   277      4.8 MiB      0.0 MiB           1           ENSEMBLE_TYPE = ensemble_type
   278      4.9 MiB      0.0 MiB           1           NUM_THREADS = num_threads
   279                                                 
   280     26.7 MiB     21.8 MiB           1           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   281     26.7 MiB      0.1 MiB           1           nb_train_seqs = len(train_data)
   282     27.3 MiB      0.6 MiB           1           seq_len = len(train_data.iloc[0, 0])
   283     27.3 MiB      0.0 MiB           1           nb_train_symbols = nb_train_seqs * seq_len
   284                                                 
   285                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   286     27.7 MiB      0.4 MiB           5           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= 0, compute_training_loss=False) for _ in unique_labels]
   287     27.8 MiB      0.0 MiB           3           for i in range(len(unique_labels)):
   288     27.8 MiB      0.1 MiB           4               spa[i].set_inference_config(
   289     27.8 MiB      0.0 MiB           2                   lb=1e-5,
   290     27.8 MiB      0.0 MiB           2                   ensemble_type= "depth",
   291     27.8 MiB      0.0 MiB           2                   ensemble_n=10,
   292     27.8 MiB      0.0 MiB           2                   backshift_parsing=True,
   293     27.8 MiB      0.0 MiB           2                   backshift_ctx_len=20,
   294     27.8 MiB      0.0 MiB           2                   backshift_break_at_phrase=True
   295                                                     )
   296                                         
   297     27.9 MiB      0.1 MiB           1           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   298     27.9 MiB      0.0 MiB           1           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   299                                         
   300     27.9 MiB      0.0 MiB           1           iterated_times = 0
   301     74.6 MiB   -462.7 MiB          11           for nb_iterations in nb_train_iterations:
   302     74.6 MiB   -313.3 MiB          10               train_one_iter_start_time = time.perf_counter()
   303     86.6 MiB   -744.9 MiB          20               for _ in range(nb_iterations - iterated_times):
   304     86.6 MiB   -416.2 MiB          10                   spa_logloss = train_spa_oneIter(train_data, spa)
   305                                                     
   306     86.6 MiB   -433.1 MiB          10               iterated_times = nb_iterations
   307     86.6 MiB  -4197.3 MiB         110               for gamma in gammas:
   308     86.6 MiB -12792.9 MiB         300                   for ensemble in ENSEMBLE_TYPE:
   309                                                         # Test on validation test to assess this combination of hyperparams
   310     38.5 MiB -11011.9 MiB         200                       validation_data = handle_N(validation_data)
   311     38.5 MiB  -9510.7 MiB         600                       for index in range(len(spa)):
   312     38.5 MiB  -6330.7 MiB         400                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   313     78.2 MiB  -1949.0 MiB         200                       accuracy = test_seq(validation_data, spa, num_threads)
   314     78.2 MiB  -8335.5 MiB         200                       train_one_iter_end_time = time.perf_counter()
   315     78.2 MiB  -8335.0 MiB         200                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   316     78.2 MiB  -8352.2 MiB         200                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   317                                         
   318                                                         
   319                                                         
   320     78.4 MiB  -8702.3 MiB         200                   current_result = pd.DataFrame([{
   321     78.2 MiB  -4052.3 MiB         100                   "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   322     78.2 MiB  -4014.6 MiB         100                   "GAMMA": gamma,
   323     78.2 MiB  -4014.8 MiB         100                   "NB_TRAIN_ITERATIONS": nb_iterations,
   324     78.2 MiB  -4014.9 MiB         100                   "HANDLE_N_SETTING": HANDLE_N_SETTING,
   325     78.2 MiB  -4015.2 MiB         100                   "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   326     78.2 MiB  -4015.1 MiB         100                   "ENSEMBLE_TYPE": ensemble,
   327     78.2 MiB  -4015.2 MiB         100                   "NUM_THREADS": NUM_THREADS,
   328     78.2 MiB  -4015.1 MiB         100                   "TRAINING_TIME": train_one_iter_duration, 
   329     78.2 MiB  -4015.1 MiB         100                   "VALIDATION ACCURACY": accuracy
   330                                                         }])
   331                                         
   332                                                         # Concatenate the current result with results_df
   333     78.5 MiB  -3698.2 MiB         100                   results_df = results_df.dropna(axis=1, how='all')
   334     78.5 MiB  -3759.3 MiB         100                   current_result = current_result.dropna(axis=1, how='all')
   335                                         
   336     78.6 MiB  -3738.0 MiB         100                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   337                                         
   338                                             
   339                                             # Find the best hyperparameter combination based on the highest accuracy
   340     23.8 MiB     -0.0 MiB           1       print("---BEST SPA(s) FOUND")
   341     24.2 MiB      0.5 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   342     24.3 MiB      0.1 MiB           1       best_params = best_row.to_dict()
   343     24.3 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   344                                         
   345                                             # Retrain and test using the best hyperparameters
   346     24.3 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   347     24.3 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   348     24.3 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   349     24.3 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   350     24.3 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   351     24.3 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   352     24.3 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   353                                         
   354                                             # Retrain our best SPAs and use that to test on test data 
   355     24.6 MiB      0.3 MiB           5       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   356     24.7 MiB      0.0 MiB           3       for i in range(len(unique_labels)):
   357     24.7 MiB      0.0 MiB           4           spa[i].set_inference_config(
   358     24.7 MiB      0.0 MiB           2               lb=1e-5,
   359     24.7 MiB      0.0 MiB           2               ensemble_type= ENSEMBLE_TYPE,
   360     24.7 MiB      0.0 MiB           2               ensemble_n=10,
   361     24.7 MiB      0.0 MiB           2               backshift_parsing=True,
   362     24.7 MiB      0.1 MiB           2               backshift_ctx_len=20,
   363     24.7 MiB      0.0 MiB           2               backshift_break_at_phrase=True
   364                                                 )
   365                                         
   366     20.6 MiB     -4.1 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   367     20.8 MiB      0.2 MiB           1       nb_train_seqs = len(train_data)
   368     22.0 MiB      1.2 MiB           1       seq_len = len(train_data.iloc[0, 0])
   369     22.0 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   370     22.1 MiB      0.1 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   371                                         
   372     22.2 MiB      0.1 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   373     19.7 MiB     -2.5 MiB           1       spa_logloss = train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   374                                         
   375     19.8 MiB      0.1 MiB           1       train_end_time = time.perf_counter()
   376     19.8 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   377                                         
   378                                             
   379                                             
   380                                             # Final test
   381     20.0 MiB      0.1 MiB           1       print("-----TESTING")
   382     20.0 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   383     29.5 MiB      9.5 MiB           1       test_data = pd.read_csv(test_path)
   384                                         
   385     29.5 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   386                                         
   387     19.8 MiB     -9.7 MiB           1       test_data = handle_N(test_data)
   388     25.0 MiB      5.1 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   389                                         
   390     25.0 MiB      0.1 MiB           1       inference_end_time = time.perf_counter()
   391     24.8 MiB     -0.2 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   392                                             
   393                                                 
   394     24.8 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   395                                         
   396     24.8 MiB      0.0 MiB           1       label = 0
   397     38.6 MiB     -0.5 MiB           3       for sp in spa:
   398     43.0 MiB     21.2 MiB           2           spa_bytes = bytearray(sp.to_bytes())
   399     43.0 MiB      0.0 MiB           2           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   400     42.9 MiB     -0.9 MiB           2           makedirs("best_spas", exist_ok=True)
   401                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   402     42.9 MiB      0.0 MiB           2           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   403                                                 
   404                                                 # Create the full path for the binary file
   405     42.9 MiB     -1.2 MiB           2           binary_file_path = os.path.join("best_spas", f"{binary_file_name}_{label}.bin")
   406     42.9 MiB      0.0 MiB           2           label += 1
   407                                                 # Save the binary file
   408     38.6 MiB     -4.9 MiB           4           with open(binary_file_path, 'wb') as file:
   409     38.6 MiB     -0.5 MiB           2               file.write(spa_bytes)
   410                                         
   411                                                 #print("Tree depth", flush=True)
   412                                                 #sp.get_tree_depth()
   413                                             
   414                                         
   415     38.6 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   416     38.6 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   417     38.6 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   418     34.4 MiB     -4.2 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   419     34.5 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   420                                             
   421                                         
   422     34.7 MiB      0.2 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   423     34.8 MiB      0.1 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   424     34.8 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   425     34.8 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   426     34.8 MiB     -0.1 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   427                                         
   428     34.8 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


