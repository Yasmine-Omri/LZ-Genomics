-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, True, remove, 0.0, entropy, 1, 15.183, 69.87
1, 0.1, True, remove, 0.0, depth, 1, 24.799, 69.04
1, 0.5, True, remove, 0.0, entropy, 1, 39.910, 69.46
1, 0.5, True, remove, 0.0, depth, 1, 47.471, 69.46
1, 0.33, True, remove, 0.0, entropy, 1, 53.803, 69.87
1, 0.33, True, remove, 0.0, depth, 1, 58.723, 69.87
1, 0.75, True, remove, 0.0, entropy, 1, 63.741, 71.97
1, 0.75, True, remove, 0.0, depth, 1, 67.373, 68.62
1, 1.0, True, remove, 0.0, entropy, 1, 71.130, 72.38
1, 1.0, True, remove, 0.0, depth, 1, 74.912, 69.04
1, 3.0, True, remove, 0.0, entropy, 1, 80.539, 74.90
1, 3.0, True, remove, 0.0, depth, 1, 85.551, 72.38
1, 5.0, True, remove, 0.0, entropy, 1, 89.143, 75.73
1, 5.0, True, remove, 0.0, depth, 1, 92.378, 73.64
3, 0.1, True, remove, 0.0, entropy, 1, 4.932, 74.06
3, 0.1, True, remove, 0.0, depth, 1, 8.351, 72.38
3, 0.5, True, remove, 0.0, entropy, 1, 11.694, 76.15
3, 0.5, True, remove, 0.0, depth, 1, 15.743, 76.15
3, 0.33, True, remove, 0.0, entropy, 1, 19.996, 74.48
3, 0.33, True, remove, 0.0, depth, 1, 23.060, 74.48
3, 0.75, True, remove, 0.0, entropy, 1, 27.202, 76.57
3, 0.75, True, remove, 0.0, depth, 1, 30.576, 78.24
3, 1.0, True, remove, 0.0, entropy, 1, 34.147, 77.41
3, 1.0, True, remove, 0.0, depth, 1, 37.411, 78.66
3, 3.0, True, remove, 0.0, entropy, 1, 41.072, 77.82
3, 3.0, True, remove, 0.0, depth, 1, 44.397, 78.24
3, 5.0, True, remove, 0.0, entropy, 1, 48.149, 78.66
3, 5.0, True, remove, 0.0, depth, 1, 51.277, 77.82
5, 0.1, True, remove, 0.0, entropy, 1, 4.545, 73.64
5, 0.1, True, remove, 0.0, depth, 1, 8.068, 74.48
5, 0.5, True, remove, 0.0, entropy, 1, 12.159, 75.73
5, 0.5, True, remove, 0.0, depth, 1, 15.946, 74.90
5, 0.33, True, remove, 0.0, entropy, 1, 19.851, 74.48
5, 0.33, True, remove, 0.0, depth, 1, 23.467, 74.48
5, 0.75, True, remove, 0.0, entropy, 1, 27.358, 75.31
5, 0.75, True, remove, 0.0, depth, 1, 30.700, 76.15
5, 1.0, True, remove, 0.0, entropy, 1, 34.822, 76.57
5, 1.0, True, remove, 0.0, depth, 1, 38.525, 76.57
5, 3.0, True, remove, 0.0, entropy, 1, 42.480, 77.82
5, 3.0, True, remove, 0.0, depth, 1, 45.930, 76.15
5, 5.0, True, remove, 0.0, entropy, 1, 49.921, 78.24
5, 5.0, True, remove, 0.0, depth, 1, 53.147, 75.31
7, 0.1, True, remove, 0.0, entropy, 1, 5.316, 77.82
7, 0.1, True, remove, 0.0, depth, 1, 9.234, 76.57
7, 0.5, True, remove, 0.0, entropy, 1, 13.609, 80.33
7, 0.5, True, remove, 0.0, depth, 1, 17.381, 78.66
7, 0.33, True, remove, 0.0, entropy, 1, 24.934, 78.66
7, 0.33, True, remove, 0.0, depth, 1, 30.878, 78.24
7, 0.75, True, remove, 0.0, entropy, 1, 42.426, 78.24
7, 0.75, True, remove, 0.0, depth, 1, 50.709, 78.66
7, 1.0, True, remove, 0.0, entropy, 1, 60.529, 77.41
7, 1.0, True, remove, 0.0, depth, 1, 70.341, 78.66
7, 3.0, True, remove, 0.0, entropy, 1, 82.292, 78.24
7, 3.0, True, remove, 0.0, depth, 1, 91.946, 76.99
7, 5.0, True, remove, 0.0, entropy, 1, 104.165, 79.08
7, 5.0, True, remove, 0.0, depth, 1, 109.222, 77.41
10, 0.1, True, remove, 0.0, entropy, 1, 9.627, 76.57
10, 0.1, True, remove, 0.0, depth, 1, 16.381, 74.48
10, 0.5, True, remove, 0.0, entropy, 1, 22.635, 76.99
10, 0.5, True, remove, 0.0, depth, 1, 27.890, 74.48
10, 0.33, True, remove, 0.0, entropy, 1, 34.351, 76.57
10, 0.33, True, remove, 0.0, depth, 1, 41.592, 74.48
10, 0.75, True, remove, 0.0, entropy, 1, 51.724, 76.99
10, 0.75, True, remove, 0.0, depth, 1, 59.371, 75.31
10, 1.0, True, remove, 0.0, entropy, 1, 66.809, 79.50
10, 1.0, True, remove, 0.0, depth, 1, 71.899, 75.73
10, 3.0, True, remove, 0.0, entropy, 1, 79.356, 79.08
10, 3.0, True, remove, 0.0, depth, 1, 86.598, 77.41
10, 5.0, True, remove, 0.0, entropy, 1, 93.233, 79.50
10, 5.0, True, remove, 0.0, depth, 1, 99.367, 77.41
th, 1, 88.359, 76.15
4, 1.5, True, remove, 0.0, entropy, 1, 104.234, 76.99
4, 2.0, True, remove, 0.0, depth, 1, 113.636, 76.99
4, 2.0, True, remove, 0.0, entropy, 1, 120.384, 76.57
4, 2.5, True, remove, 0.0, depth, 1, 126.183, 76.99
4, 2.5, True, remove, 0.0, entropy, 1, 131.288, 78.24
4, 3.0, True, remove, 0.0, depth, 1, 135.807, 76.99
4, 3.0, True, remove, 0.0, entropy, 1, 141.719, 78.66
4, 5.0, True, remove, 0.0, depth, 1, 148.405, 78.24
4, 5.0, True, remove, 0.0, entropy, 1, 152.722, 79.08
5, 0.1, True, remove, 0.0, depth, 1, 4.820, 74.48
5, 0.1, True, remove, 0.0, entropy, 1, 9.092, 73.64
5, 0.5, True, remove, 0.0, depth, 1, 12.768, 74.90
5, 0.5, True, remove, 0.0, entropy, 1, 17.182, 75.73
5, 0.33, True, remove, 0.0, depth, 1, 21.698, 74.48
5, 0.33, True, remove, 0.0, entropy, 1, 25.692, 74.48
5, 0.75, True, remove, 0.0, depth, 1, 29.898, 76.15
5, 0.75, True, remove, 0.0, entropy, 1, 33.982, 75.31
5, 1.0, True, remove, 0.0, depth, 1, 37.539, 76.57
5, 1.0, True, remove, 0.0, entropy, 1, 41.189, 76.57
5, 1.5, True, remove, 0.0, depth, 1, 44.929, 77.41
5, 1.5, True, remove, 0.0, entropy, 1, 49.105, 77.41
5, 2.0, True, remove, 0.0, depth, 1, 52.889, 76.99
5, 2.0, True, remove, 0.0, entropy, 1, 56.936, 77.82
5, 2.5, True, remove, 0.0, depth, 1, 60.720, 76.57
5, 2.5, True, remove, 0.0, entropy, 1, 64.487, 77.82
5, 3.0, True, remove, 0.0, depth, 1, 68.330, 76.15
5, 3.0, True, remove, 0.0, entropy, 1, 72.040, 77.82
5, 5.0, True, remove, 0.0, depth, 1, 75.848, 75.31
5, 5.0, True, remove, 0.0, entropy, 1, 79.426, 78.24
6, 0.1, True, remove, 0.0, depth, 1, 4.100, 75.31
6, 0.1, True, remove, 0.0, entropy, 1, 8.269, 76.15
6, 0.5, True, remove, 0.0, depth, 1, 12.157, 78.24
6, 0.5, True, remove, 0.0, entropy, 1, 15.879, 79.08
6, 0.33, True, remove, 0.0, depth, 1, 19.717, 76.99
6, 0.33, True, remove, 0.0, entropy, 1, 23.570, 78.24
6, 0.75, True, remove, 0.0, depth, 1, 27.390, 79.50
6, 0.75, True, remove, 0.0, entropy, 1, 31.192, 79.50
6, 1.0, True, remove, 0.0, depth, 1, 35.078, 79.50
6, 1.0, True, remove, 0.0, entropy, 1, 39.650, 79.08
6, 1.5, True, remove, 0.0, depth, 1, 43.439, 78.66
6, 1.5, True, remove, 0.0, entropy, 1, 48.680, 78.66
6, 2.0, True, remove, 0.0, depth, 1, 54.402, 78.66
6, 2.0, True, remove, 0.0, entropy, 1, 63.528, 79.08
6, 2.5, True, remove, 0.0, depth, 1, 73.163, 79.50
6, 2.5, True, remove, 0.0, entropy, 1, 81.686, 78.24
6, 3.0, True, remove, 0.0, depth, 1, 91.396, 79.50
6, 3.0, True, remove, 0.0, entropy, 1, 101.752, 78.66
6, 5.0, True, remove, 0.0, depth, 1, 112.191, 77.82
6, 5.0, True, remove, 0.0, entropy, 1, 122.422, 79.08
7, 0.1, True, remove, 0.0, depth, 1, 10.746, 76.57
7, 0.1, True, remove, 0.0, entropy, 1, 19.665, 77.82
7, 0.5, True, remove, 0.0, depth, 1, 26.840, 78.66
7, 0.5, True, remove, 0.0, entropy, 1, 33.700, 80.33
7, 0.33, True, remove, 0.0, depth, 1, 39.427, 78.24
7, 0.33, True, remove, 0.0, entropy, 1, 45.345, 78.66
7, 0.75, True, remove, 0.0, depth, 1, 50.810, 78.66
7, 0.75, True, remove, 0.0, entropy, 1, 59.719, 78.24
7, 1.0, True, remove, 0.0, depth, 1, 67.633, 78.66
7, 1.0, True, remove, 0.0, entropy, 1, 76.249, 77.41
7, 1.5, True, remove, 0.0, depth, 1, 81.092, 78.24
7, 1.5, True, remove, 0.0, entropy, 1, 86.177, 78.24
7, 2.0, True, remove, 0.0, depth, 1, 93.910, 76.99
7, 2.0, True, remove, 0.0, entropy, 1, 100.640, 78.24
7, 2.5, True, remove, 0.0, depth, 1, 106.204, 76.99
7, 2.5, True, remove, 0.0, entropy, 1, 111.900, 78.24
7, 3.0, True, remove, 0.0, depth, 1, 118.077, 76.99
7, 3.0, True, remove, 0.0, entropy, 1, 124.948, 78.24
7, 5.0, True, remove, 0.0, depth, 1, 131.845, 77.41
7, 5.0, True, remove, 0.0, entropy, 1, 136.440, 79.08
8, 0.1, True, remove, 0.0, depth, 1, 6.152, 76.99
8, 0.1, True, remove, 0.0, entropy, 1, 14.382, 76.99
8, 0.5, True, remove, 0.0, depth, 1, 20.360, 78.66
8, 0.5, True, remove, 0.0, entropy, 1, 27.035, 79.08
8, 0.33, True, remove, 0.0, depth, 1, 34.750, 78.24
8, 0.33, True, remove, 0.0, entropy, 1, 44.413, 79.08
8, 0.75, True, remove, 0.0, depth, 1, 53.747, 77.41
8, 0.75, True, remove, 0.0, entropy, 1, 60.880, 79.92
8, 1.0, True, remove, 0.0, depth, 1, 66.070, 77.41
8, 1.0, True, remove, 0.0, entropy, 1, 70.986, 79.50
8, 1.5, True, remove, 0.0, depth, 1, 75.160, 76.57
8, 1.5, True, remove, 0.0, entropy, 1, 79.907, 79.50
8, 2.0, True, remove, 0.0, depth, 1, 85.301, 76.99
8, 2.0, True, remove, 0.0, entropy, 1, 89.505, 79.92
8, 2.5, True, remove, 0.0, depth, 1, 93.637, 77.82
8, 2.5, True, remove, 0.0, entropy, 1, 97.842, 79.50
8, 3.0, True, remove, 0.0, depth, 1, 102.256, 77.41
8, 3.0, True, remove, 0.0, entropy, 1, 106.958, 79.50
8, 5.0, True, remove, 0.0, depth, 1, 110.811, 75.73
8, 5.0, True, remove, 0.0, entropy, 1, 115.137, 79.08
9, 0.1, True, remove, 0.0, depth, 1, 4.661, 75.31
9, 0.1, True, remove, 0.0, entropy, 1, 9.280, 76.57
9, 0.5, True, remove, 0.0, depth, 1, 13.761, 76.99
9, 0.5, True, remove, 0.0, entropy, 1, 18.769, 78.24
9, 0.33, True, remove, 0.0, depth, 1, 23.246, 76.15
9, 0.33, True, remove, 0.0, entropy, 1, 28.245, 77.41
9, 0.75, True, remove, 0.0, depth, 1, 33.296, 76.57
9, 0.75, True, remove, 0.0, entropy, 1, 37.728, 77.82
9, 1.0, True, remove, 0.0, depth, 1, 41.829, 76.99
9, 1.0, True, remove, 0.0, entropy, 1, 46.487, 78.24
9, 1.5, True, remove, 0.0, depth, 1, 51.304, 77.82
9, 1.5, True, remove, 0.0, entropy, 1, 56.129, 78.66
9, 2.0, True, remove, 0.0, depth, 1, 60.297, 77.41
9, 2.0, True, remove, 0.0, entropy, 1, 64.834, 78.66
9, 2.5, True, remove, 0.0, depth, 1, 69.506, 77.41
9, 2.5, True, remove, 0.0, entropy, 1, 74.295, 78.24
9, 3.0, True, remove, 0.0, depth, 1, 79.209, 77.41
9, 3.0, True, remove, 0.0, entropy, 1, 84.007, 77.82
9, 5.0, True, remove, 0.0, depth, 1, 90.258, 77.82
9, 5.0, True, remove, 0.0, entropy, 1, 94.904, 79.50
10, 0.1, True, remove, 0.0, depth, 1, 5.986, 74.48
10, 0.1, True, remove, 0.0, entropy, 1, 11.245, 76.57
10, 0.5, True, remove, 0.0, depth, 1, 16.325, 74.48
10, 0.5, True, remove, 0.0, entropy, 1, 22.079, 76.99
10, 0.33, True, remove, 0.0, depth, 1, 27.524, 74.48
10, 0.33, True, remove, 0.0, entropy, 1, 32.796, 76.57
10, 0.75, True, remove, 0.0, depth, 1, 37.936, 75.31
10, 0.75, True, remove, 0.0, entropy, 1, 42.909, 76.99
10, 1.0, True, remove, 0.0, depth, 1, 47.736, 75.73
10, 1.0, True, remove, 0.0, entropy, 1, 53.002, 79.50
10, 1.5, True, remove, 0.0, depth, 1, 58.016, 77.41
10, 1.5, True, remove, 0.0, entropy, 1, 63.506, 79.50
10, 2.0, True, remove, 0.0, depth, 1, 68.636, 76.99
10, 2.0, True, remove, 0.0, entropy, 1, 73.920, 78.66
10, 2.5, True, remove, 0.0, depth, 1, 79.032, 76.99
10, 2.5, True, remove, 0.0, entropy, 1, 84.896, 79.50
10, 3.0, True, remove, 0.0, depth, 1, 89.460, 77.41
10, 3.0, True, remove, 0.0, entropy, 1, 94.342, 79.08
10, 5.0, True, remove, 0.0, depth, 1, 99.592, 77.41
10, 5.0, True, remove, 0.0, entropy, 1, 105.011, 79.50
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': True, 'GAMMA': 0.5, 'NB_TRAIN_ITERATIONS': 7, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.0, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 1, 'TRAINING_TIME': 33.700123791000806, 'VALIDATION ACCURACY': 0.803347280334728}
-----TESTING
Final accuracy with best hyperparameters: 77.82
Mem in MB: 2.18
Mem in MB: 2.12
-----TIME PROFILING+
Read train + val data time:  3.28027
Number of training symbols: 192304
Length of one training sequence: 101
Total training time: 949.815 seconds
Number of test sequences: 239
Length of test sequence: 101
Read test data time:  0.08184
Total inference time: 5.297 seconds
Inference time/symbol: 0.00021944100658738668 seconds
-----MEMORY REPORT
Filename: /Users/eugenemin/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   223     36.5 MiB     36.5 MiB           1   @profile
   224                                         def main(dataset_folder, pretrain_file):
   225                                             global INCLUDE_PREV_CONTEXT
   226                                             global GAMMA
   227                                             global NB_TRAIN_ITERATIONS 
   228                                             global HANDLE_N_SETTING 
   229                                             global RATIO_PRETRAIN_TRAIN 
   230                                             global ENSEMBLE_TYPE 
   231                                             global NUM_THREADS
   232                                             
   233                                             global include_prev_contexts
   234                                             global gammas 
   235                                             global nb_train_iterations 
   236                                             global handle_N_settings 
   237                                             global ratio_pretrain_train
   238                                             global ensemble_type
   239                                             global num_threads
   240                                         
   241     36.6 MiB      0.1 MiB           1       read_data_in_time = time.perf_counter()
   242                                             
   243                                             # Read train, val, test data 
   244     36.6 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   245     36.6 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   246     36.6 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   247                                             
   248                                         
   249     45.2 MiB      8.6 MiB           1       train_data = pd.read_csv(train_path)
   250     36.6 MiB     -8.6 MiB           1       validation_data = pd.read_csv(val_path)
   251                                             
   252     36.7 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   253     38.1 MiB      1.5 MiB           1       unique_labels = train_data['label'].unique()
   254                                             
   255     35.1 MiB     -4.7 MiB           2       with open(pretrain_file, 'r') as file:
   256     15.8 MiB    -19.3 MiB           1           pretrain_data = file.read()
   257                                             
   258                                             # Train all SPAs using all possible combinations of hyperparams
   259                                             # Test all on validation set, return best SPA
   260      6.3 MiB    -28.8 MiB           1       results_df = pd.DataFrame(columns=[
   261                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   262                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   263                                             ])
   264                                         
   265      7.0 MiB      0.7 MiB           1       print("-----TRAINING")
   266      7.0 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   267      7.2 MiB      0.2 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   268      7.3 MiB      0.1 MiB           1       train_start_time = time.perf_counter()
   269     30.4 MiB     -6.9 MiB           3       for include_prev_context, handle_N_setting, ratio in itertools.product(
   270      7.4 MiB      0.0 MiB           1       include_prev_contexts, handle_N_settings, ratio_pretrain_train
   271                                             ):  
   272      7.4 MiB      0.0 MiB           1           INCLUDE_PREV_CONTEXT = include_prev_context
   273      7.4 MiB      0.0 MiB           1           GAMMA = gammas
   274      7.4 MiB      0.0 MiB           1           NB_TRAIN_ITERATIONS = 0
   275      7.4 MiB      0.0 MiB           1           HANDLE_N_SETTING = handle_N_setting
   276      7.6 MiB      0.1 MiB           1           RATIO_PRETRAIN_TRAIN = ratio 
   277      7.6 MiB      0.0 MiB           1           ENSEMBLE_TYPE = ensemble_type
   278      7.6 MiB      0.0 MiB           1           NUM_THREADS = num_threads
   279                                                 
   280     19.7 MiB     12.1 MiB           1           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   281     19.8 MiB      0.2 MiB           1           nb_train_seqs = len(train_data)
   282     20.5 MiB      0.7 MiB           1           seq_len = len(train_data.iloc[0, 0])
   283     20.5 MiB      0.0 MiB           1           nb_train_symbols = nb_train_seqs * seq_len
   284                                                 
   285                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   286     20.9 MiB      0.4 MiB           5           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= 0, compute_training_loss=False) for _ in unique_labels]
   287     21.0 MiB      0.0 MiB           3           for i in range(len(unique_labels)):
   288     21.0 MiB      0.1 MiB           4               spa[i].set_inference_config(
   289     21.0 MiB      0.0 MiB           2                   lb=1e-5,
   290     21.0 MiB      0.0 MiB           2                   ensemble_type= "depth",
   291     21.0 MiB      0.0 MiB           2                   ensemble_n=10,
   292     21.0 MiB      0.0 MiB           2                   backshift_parsing=True,
   293     21.0 MiB      0.0 MiB           2                   backshift_ctx_len=20,
   294     21.0 MiB      0.0 MiB           2                   backshift_break_at_phrase=True
   295                                                     )
   296                                         
   297     21.1 MiB      0.1 MiB           1           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   298     21.2 MiB      0.1 MiB           1           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   299                                         
   300     21.2 MiB      0.0 MiB           1           iterated_times = 0
   301     37.4 MiB   -194.4 MiB          11           for nb_iterations in nb_train_iterations:
   302     37.4 MiB    -27.4 MiB          10               train_one_iter_start_time = time.perf_counter()
   303     39.9 MiB    -56.4 MiB          20               for _ in range(nb_iterations - iterated_times):
   304     39.9 MiB    -18.4 MiB          10                   spa_logloss = train_spa_oneIter(train_data, spa)
   305                                                     
   306     39.9 MiB    -29.1 MiB          10               iterated_times = nb_iterations
   307     68.3 MiB  -1863.5 MiB         110               for gamma in gammas:
   308     68.3 MiB  -6092.7 MiB         300                   for ensemble in ENSEMBLE_TYPE:
   309                                                         # Test on validation test to assess this combination of hyperparams
   310     40.8 MiB  -4654.8 MiB         200                       validation_data = handle_N(validation_data)
   311     40.8 MiB  -7504.5 MiB         600                       for index in range(len(spa)):
   312     40.8 MiB  -4997.2 MiB         400                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   313     67.0 MiB  -2367.4 MiB         200                       accuracy = test_seq(validation_data, spa, num_threads)
   314     67.0 MiB  -4334.1 MiB         200                       train_one_iter_end_time = time.perf_counter()
   315     67.0 MiB  -4333.9 MiB         200                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   316     67.0 MiB  -4324.0 MiB         200                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   317                                         
   318                                                         
   319                                                         
   320     67.7 MiB  -4147.1 MiB         200                   current_result = pd.DataFrame([{
   321     67.0 MiB  -2153.2 MiB         100                   "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   322     67.0 MiB  -2122.7 MiB         100                   "GAMMA": gamma,
   323     67.0 MiB  -2122.7 MiB         100                   "NB_TRAIN_ITERATIONS": nb_iterations,
   324     67.0 MiB  -2122.6 MiB         100                   "HANDLE_N_SETTING": HANDLE_N_SETTING,
   325     67.0 MiB  -2122.7 MiB         100                   "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   326     67.0 MiB  -2122.7 MiB         100                   "ENSEMBLE_TYPE": ensemble,
   327     67.0 MiB  -2122.7 MiB         100                   "NUM_THREADS": NUM_THREADS,
   328     67.0 MiB  -2122.7 MiB         100                   "TRAINING_TIME": train_one_iter_duration, 
   329     67.0 MiB  -2122.7 MiB         100                   "VALIDATION ACCURACY": accuracy
   330                                                         }])
   331                                         
   332                                                         # Concatenate the current result with results_df
   333     68.1 MiB  -1770.6 MiB         100                   results_df = results_df.dropna(axis=1, how='all')
   334     68.1 MiB  -1798.6 MiB         100                   current_result = current_result.dropna(axis=1, how='all')
   335                                         
   336     68.3 MiB  -1778.0 MiB         100                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   337                                         
   338                                             
   339                                             # Find the best hyperparameter combination based on the highest accuracy
   340     30.4 MiB      0.0 MiB           1       print("---BEST SPA(s) FOUND")
   341     31.1 MiB      0.7 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   342     31.1 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   343     31.2 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   344                                         
   345                                             # Retrain and test using the best hyperparameters
   346     31.2 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   347     31.2 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   348     31.2 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   349     31.2 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   350     31.2 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   351     31.2 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   352     31.2 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   353                                         
   354                                             # Retrain our best SPAs and use that to test on test data 
   355     31.4 MiB      0.2 MiB           5       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   356     31.4 MiB      0.0 MiB           3       for i in range(len(unique_labels)):
   357     31.4 MiB      0.0 MiB           4           spa[i].set_inference_config(
   358     31.4 MiB      0.0 MiB           2               lb=1e-5,
   359     31.4 MiB      0.0 MiB           2               ensemble_type= ENSEMBLE_TYPE,
   360     31.4 MiB      0.0 MiB           2               ensemble_n=10,
   361     31.4 MiB      0.0 MiB           2               backshift_parsing=True,
   362     31.4 MiB      0.0 MiB           2               backshift_ctx_len=20,
   363     31.4 MiB      0.0 MiB           2               backshift_break_at_phrase=True
   364                                                 )
   365                                         
   366     22.2 MiB     -9.2 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   367     22.3 MiB      0.1 MiB           1       nb_train_seqs = len(train_data)
   368     22.8 MiB      0.5 MiB           1       seq_len = len(train_data.iloc[0, 0])
   369     22.8 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   370     22.9 MiB      0.1 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   371                                         
   372     22.9 MiB      0.1 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   373     23.9 MiB      1.0 MiB           1       spa_logloss = train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   374                                         
   375     24.1 MiB      0.2 MiB           1       train_end_time = time.perf_counter()
   376     24.1 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   377                                         
   378                                             
   379                                             
   380                                             # Final test
   381     24.2 MiB      0.1 MiB           1       print("-----TESTING")
   382     24.2 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   383     39.6 MiB     15.4 MiB           1       test_data = pd.read_csv(test_path)
   384                                         
   385     39.6 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   386                                         
   387     27.2 MiB    -12.4 MiB           1       test_data = handle_N(test_data)
   388     28.8 MiB      1.5 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   389                                         
   390     28.9 MiB      0.1 MiB           1       inference_end_time = time.perf_counter()
   391     29.0 MiB      0.1 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   392                                             
   393                                                 
   394     29.0 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   395                                         
   396     29.0 MiB      0.0 MiB           1       label = 0
   397     52.2 MiB      0.0 MiB           3       for sp in spa:
   398     52.2 MiB     22.8 MiB           2           spa_bytes = bytearray(sp.to_bytes())
   399     52.2 MiB      0.1 MiB           2           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   400     52.2 MiB      0.3 MiB           2           makedirs("best_spas", exist_ok=True)
   401                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   402     52.2 MiB      0.0 MiB           2           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   403                                                 
   404                                                 # Create the full path for the binary file
   405     52.2 MiB      0.0 MiB           2           binary_file_path = os.path.join("best_spas", f"{binary_file_name}_{label}.bin")
   406     52.2 MiB      0.0 MiB           2           label += 1
   407                                                 # Save the binary file
   408     52.2 MiB      0.1 MiB           4           with open(binary_file_path, 'wb') as file:
   409     52.2 MiB      0.0 MiB           2               file.write(spa_bytes)
   410                                         
   411                                                 #print("Tree depth", flush=True)
   412                                                 #sp.get_tree_depth()
   413                                             
   414                                         
   415     52.2 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   416     52.2 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   417     52.2 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   418     52.7 MiB      0.5 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   419     52.7 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   420                                             
   421                                         
   422     52.7 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   423     52.8 MiB      0.1 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   424     52.8 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   425     52.8 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   426     52.8 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   427                                         
   428     52.8 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


