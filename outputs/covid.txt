-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, False, remove, 0.0, depth, 48, 57.128, 61.56
1, 0.1, False, remove, 0.0, entropy, 48, 103.130, 63.67
1, 0.5, False, remove, 0.0, depth, 48, 147.057, 44.40
1, 0.5, False, remove, 0.0, entropy, 48, 192.925, 49.41
1, 0.33, False, remove, 0.0, depth, 48, 236.817, 49.73
1, 0.33, False, remove, 0.0, entropy, 48, 283.278, 54.71
1, 0.75, False, remove, 0.0, depth, 48, 327.270, 39.34
1, 0.75, False, remove, 0.0, entropy, 48, 373.171, 44.56
1, 1.0, False, remove, 0.0, depth, 48, 417.092, 35.01
1, 1.0, False, remove, 0.0, entropy, 48, 462.902, 40.92
1, 3.0, False, remove, 0.0, depth, 48, 506.914, 22.97
1, 3.0, False, remove, 0.0, entropy, 48, 552.834, 25.74
1, 5.0, False, remove, 0.0, depth, 48, 596.727, 20.16
1, 5.0, False, remove, 0.0, entropy, 48, 642.670, 22.49
3, 0.1, False, remove, 0.0, depth, 48, 82.307, 69.47
3, 0.1, False, remove, 0.0, entropy, 48, 133.340, 69.92
3, 0.5, False, remove, 0.0, depth, 48, 182.277, 56.32
3, 0.5, False, remove, 0.0, entropy, 48, 233.080, 58.03
3, 0.33, False, remove, 0.0, depth, 48, 281.971, 60.98
3, 0.33, False, remove, 0.0, entropy, 48, 332.982, 62.37
3, 0.75, False, remove, 0.0, depth, 48, 381.907, 51.08
3, 0.75, False, remove, 0.0, entropy, 48, 432.826, 53.24
3, 1.0, False, remove, 0.0, depth, 48, 481.675, 46.25
3, 1.0, False, remove, 0.0, entropy, 48, 532.603, 49.41
3, 3.0, False, remove, 0.0, depth, 48, 581.581, 30.08
3, 3.0, False, remove, 0.0, entropy, 48, 632.435, 32.51
3, 5.0, False, remove, 0.0, depth, 48, 681.303, 24.01
3, 5.0, False, remove, 0.0, entropy, 48, 732.134, 26.12
5, 0.1, False, remove, 0.0, depth, 48, 90.892, 71.34
5, 0.1, False, remove, 0.0, entropy, 48, 146.339, 71.79
5, 0.5, False, remove, 0.0, depth, 48, 199.618, 58.68
5, 0.5, False, remove, 0.0, entropy, 48, 255.007, 59.69
5, 0.33, False, remove, 0.0, depth, 48, 308.385, 63.14
5, 0.33, False, remove, 0.0, entropy, 48, 363.682, 63.96
5, 0.75, False, remove, 0.0, depth, 48, 416.955, 53.48
5, 0.75, False, remove, 0.0, entropy, 48, 472.275, 55.06
5, 1.0, False, remove, 0.0, depth, 48, 525.521, 49.48
5, 1.0, False, remove, 0.0, entropy, 48, 580.799, 50.77
5, 3.0, False, remove, 0.0, depth, 48, 634.118, 31.58
5, 3.0, False, remove, 0.0, entropy, 48, 689.462, 33.67
5, 5.0, False, remove, 0.0, depth, 48, 742.691, 24.63
5, 5.0, False, remove, 0.0, entropy, 48, 798.065, 26.35
7, 0.1, False, remove, 0.0, depth, 48, 96.625, 72.17
7, 0.1, False, remove, 0.0, entropy, 48, 156.880, 72.06
7, 0.5, False, remove, 0.0, depth, 48, 215.118, 59.74
7, 0.5, False, remove, 0.0, entropy, 48, 275.438, 60.45
7, 0.33, False, remove, 0.0, depth, 48, 333.912, 63.80
7, 0.33, False, remove, 0.0, entropy, 48, 394.303, 63.81
7, 0.75, False, remove, 0.0, depth, 48, 452.620, 54.81
7, 0.75, False, remove, 0.0, entropy, 48, 512.744, 55.77
7, 1.0, False, remove, 0.0, depth, 48, 571.024, 50.63
7, 1.0, False, remove, 0.0, entropy, 48, 633.160, 51.51
7, 3.0, False, remove, 0.0, depth, 48, 691.446, 32.99
7, 3.0, False, remove, 0.0, entropy, 48, 751.757, 34.25
7, 5.0, False, remove, 0.0, depth, 48, 809.934, 25.58
7, 5.0, False, remove, 0.0, entropy, 48, 870.288, 26.86
10, 0.1, False, remove, 0.0, depth, 48, 127.600, 73.12
10, 0.1, False, remove, 0.0, entropy, 48, 196.750, 73.06
10, 0.5, False, remove, 0.0, depth, 48, 263.911, 60.57
10, 0.5, False, remove, 0.0, entropy, 48, 333.104, 60.70
10, 0.33, False, remove, 0.0, depth, 48, 399.976, 64.53
10, 0.33, False, remove, 0.0, entropy, 48, 469.133, 64.74
10, 0.75, False, remove, 0.0, depth, 48, 536.142, 55.45
10, 0.75, False, remove, 0.0, entropy, 48, 605.217, 55.98
10, 1.0, False, remove, 0.0, depth, 48, 672.241, 51.41
10, 1.0, False, remove, 0.0, entropy, 48, 741.270, 52.06
10, 3.0, False, remove, 0.0, depth, 48, 808.324, 34.01
10, 3.0, False, remove, 0.0, entropy, 48, 877.473, 35.06
10, 5.0, False, remove, 0.0, depth, 48, 944.370, 26.17
10, 5.0, False, remove, 0.0, entropy, 48, 1013.284, 28.15
1, 0.1, False, remove, 0.25, depth, 48, 64.327, 62.94
1, 0.1, False, remove, 0.25, entropy, 48, 112.651, 65.04
1, 0.5, False, remove, 0.25, depth, 48, 158.935, 46.43
1, 0.5, False, remove, 0.25, entropy, 48, 207.234, 52.07
1, 0.33, False, remove, 0.25, depth, 48, 253.555, 51.82
1, 0.33, False, remove, 0.25, entropy, 48, 302.026, 56.59
1, 0.75, False, remove, 0.25, depth, 48, 348.347, 40.92
1, 0.75, False, remove, 0.25, entropy, 48, 396.696, 46.83
1, 1.0, False, remove, 0.25, depth, 48, 442.982, 37.49
1, 1.0, False, remove, 0.25, entropy, 48, 491.220, 43.09
1, 3.0, False, remove, 0.25, depth, 48, 537.596, 24.83
1, 3.0, False, remove, 0.25, entropy, 48, 585.909, 29.13
1, 5.0, False, remove, 0.25, depth, 48, 632.121, 21.58
1, 5.0, False, remove, 0.25, entropy, 48, 680.515, 24.61
3, 0.1, False, remove, 0.25, depth, 48, 91.351, 70.51
3, 0.1, False, remove, 0.25, entropy, 48, 144.044, 70.81
3, 0.5, False, remove, 0.25, depth, 48, 194.805, 57.72
3, 0.5, False, remove, 0.25, entropy, 48, 247.524, 59.58
3, 0.33, False, remove, 0.25, depth, 48, 298.213, 61.84
3, 0.33, False, remove, 0.25, entropy, 48, 350.893, 63.79
3, 0.75, False, remove, 0.25, depth, 48, 401.698, 52.20
3, 0.75, False, remove, 0.25, entropy, 48, 454.301, 54.39
3, 1.0, False, remove, 0.25, depth, 48, 504.958, 47.09
3, 1.0, False, remove, 0.25, entropy, 48, 557.695, 50.17
3, 3.0, False, remove, 0.25, depth, 48, 608.433, 30.82
3, 3.0, False, remove, 0.25, entropy, 48, 661.165, 33.30
3, 5.0, False, remove, 0.25, depth, 48, 711.903, 25.25
3, 5.0, False, remove, 0.25, entropy, 48, 764.666, 27.07
5, 0.1, False, remove, 0.25, depth, 48, 97.181, 72.17
5, 0.1, False, remove, 0.25, entropy, 48, 154.505, 72.50
5, 0.5, False, remove, 0.25, depth, 48, 209.804, 59.45
5, 0.5, False, remove, 0.25, entropy, 48, 267.036, 60.60
5, 0.33, False, remove, 0.25, depth, 48, 322.286, 63.58
5, 0.33, False, remove, 0.25, entropy, 48, 379.724, 64.47
5, 0.75, False, remove, 0.25, depth, 48, 435.160, 53.94
5, 0.75, False, remove, 0.25, entropy, 48, 492.462, 55.79
5, 1.0, False, remove, 0.25, depth, 48, 548.505, 49.60
5, 1.0, False, remove, 0.25, entropy, 48, 605.923, 51.23
5, 3.0, False, remove, 0.25, depth, 48, 661.219, 32.51
5, 3.0, False, remove, 0.25, entropy, 48, 718.509, 34.54
5, 5.0, False, remove, 0.25, depth, 48, 773.776, 25.98
5, 5.0, False, remove, 0.25, entropy, 48, 831.677, 27.38
7, 0.1, False, remove, 0.25, depth, 48, 104.046, 72.86
7, 0.1, False, remove, 0.25, entropy, 48, 167.036, 73.10
7, 0.5, False, remove, 0.25, depth, 48, 227.995, 60.32
7, 0.5, False, remove, 0.25, entropy, 48, 290.884, 61.08
7, 0.33, False, remove, 0.25, depth, 48, 351.676, 64.58
7, 0.33, False, remove, 0.25, entropy, 48, 414.581, 64.89
7, 0.75, False, remove, 0.25, depth, 48, 475.276, 55.30
7, 0.75, False, remove, 0.25, entropy, 48, 538.161, 56.20
7, 1.0, False, remove, 0.25, depth, 48, 599.018, 51.35
7, 1.0, False, remove, 0.25, entropy, 48, 661.974, 52.53
7, 3.0, False, remove, 0.25, depth, 48, 722.797, 34.28
7, 3.0, False, remove, 0.25, entropy, 48, 785.677, 36.09
7, 5.0, False, remove, 0.25, depth, 48, 846.807, 27.25
7, 5.0, False, remove, 0.25, entropy, 48, 909.862, 28.64
10, 0.1, False, remove, 0.25, depth, 48, 135.517, 73.40
10, 0.1, False, remove, 0.25, entropy, 48, 207.791, 73.39
10, 0.5, False, remove, 0.25, depth, 48, 277.658, 60.41
10, 0.5, False, remove, 0.25, entropy, 48, 349.711, 60.87
10, 0.33, False, remove, 0.25, depth, 48, 419.750, 64.78
10, 0.33, False, remove, 0.25, entropy, 48, 491.798, 64.89
10, 0.75, False, remove, 0.25, depth, 48, 561.746, 55.64
10, 0.75, False, remove, 0.25, entropy, 48, 633.859, 55.91
10, 1.0, False, remove, 0.25, depth, 48, 703.880, 51.43
10, 1.0, False, remove, 0.25, entropy, 48, 775.694, 52.36
10, 3.0, False, remove, 0.25, depth, 48, 845.561, 33.98
10, 3.0, False, remove, 0.25, entropy, 48, 917.655, 35.64
10, 5.0, False, remove, 0.25, depth, 48, 987.525, 26.54
10, 5.0, False, remove, 0.25, entropy, 48, 1059.483, 28.05
1, 0.1, False, remove, 0.1, depth, 48, 61.828, 62.75
1, 0.1, False, remove, 0.1, entropy, 48, 109.341, 64.87
1, 0.5, False, remove, 0.1, depth, 48, 154.748, 44.96
1, 0.5, False, remove, 0.1, entropy, 48, 202.230, 50.63
1, 0.33, False, remove, 0.1, depth, 48, 247.624, 50.58
1, 0.33, False, remove, 0.1, entropy, 48, 295.229, 55.79
1, 0.75, False, remove, 0.1, depth, 48, 340.707, 39.28
1, 0.75, False, remove, 0.1, entropy, 48, 388.081, 44.81
1, 1.0, False, remove, 0.1, depth, 48, 433.525, 35.73
1, 1.0, False, remove, 0.1, entropy, 48, 480.891, 41.04
1, 3.0, False, remove, 0.1, depth, 48, 526.354, 23.94
1, 3.0, False, remove, 0.1, entropy, 48, 573.794, 27.56
1, 5.0, False, remove, 0.1, depth, 48, 619.160, 20.27
1, 5.0, False, remove, 0.1, entropy, 48, 666.476, 23.79
3, 0.1, False, remove, 0.1, depth, 48, 88.126, 70.22
3, 0.1, False, remove, 0.1, entropy, 48, 140.855, 70.87
3, 0.5, False, remove, 0.1, depth, 48, 191.511, 56.32
3, 0.5, False, remove, 0.1, entropy, 48, 244.299, 58.38
3, 0.33, False, remove, 0.1, depth, 48, 295.027, 61.56
3, 0.33, False, remove, 0.1, entropy, 48, 347.767, 63.11
3, 0.75, False, remove, 0.1, depth, 48, 398.392, 50.94
3, 0.75, False, remove, 0.1, entropy, 48, 451.071, 53.10
3, 1.0, False, remove, 0.1, depth, 48, 502.181, 46.29
3, 1.0, False, remove, 0.1, entropy, 48, 555.297, 48.89
3, 3.0, False, remove, 0.1, depth, 48, 606.073, 30.54
3, 3.0, False, remove, 0.1, entropy, 48, 658.746, 32.62
3, 5.0, False, remove, 0.1, depth, 48, 709.419, 24.53
3, 5.0, False, remove, 0.1, entropy, 48, 762.062, 26.31
5, 0.1, False, remove, 0.1, depth, 48, 95.493, 72.30
5, 0.1, False, remove, 0.1, entropy, 48, 152.371, 72.63
5, 0.5, False, remove, 0.1, depth, 48, 207.165, 59.64
5, 0.5, False, remove, 0.1, entropy, 48, 264.012, 60.80
5, 0.33, False, remove, 0.1, depth, 48, 318.769, 63.82
5, 0.33, False, remove, 0.1, entropy, 48, 375.538, 64.53
5, 0.75, False, remove, 0.1, depth, 48, 430.358, 54.59
5, 0.75, False, remove, 0.1, entropy, 48, 487.110, 56.10
5, 1.0, False, remove, 0.1, depth, 48, 541.854, 50.70
5, 1.0, False, remove, 0.1, entropy, 48, 598.588, 52.07
5, 3.0, False, remove, 0.1, depth, 48, 653.260, 33.40
5, 3.0, False, remove, 0.1, entropy, 48, 710.004, 35.48
5, 5.0, False, remove, 0.1, depth, 48, 764.801, 26.78
5, 5.0, False, remove, 0.1, entropy, 48, 821.550, 28.54
7, 0.1, False, remove, 0.1, depth, 48, 101.815, 73.22
7, 0.1, False, remove, 0.1, entropy, 48, 163.411, 73.40
7, 0.5, False, remove, 0.1, depth, 48, 222.906, 60.55
7, 0.5, False, remove, 0.1, entropy, 48, 284.295, 61.03
7, 0.33, False, remove, 0.1, depth, 48, 344.263, 64.70
7, 0.33, False, remove, 0.1, entropy, 48, 405.896, 65.24
7, 0.75, False, remove, 0.1, depth, 48, 465.817, 55.44
7, 0.75, False, remove, 0.1, entropy, 48, 527.327, 56.36
7, 1.0, False, remove, 0.1, depth, 48, 586.952, 51.18
7, 1.0, False, remove, 0.1, entropy, 48, 650.652, 52.18
7, 3.0, False, remove, 0.1, depth, 48, 711.090, 33.31
7, 3.0, False, remove, 0.1, entropy, 48, 772.763, 35.39
7, 5.0, False, remove, 0.1, depth, 48, 833.353, 26.03
7, 5.0, False, remove, 0.1, entropy, 48, 894.861, 27.74
10, 0.1, False, remove, 0.1, depth, 48, 131.626, 73.43
10, 0.1, False, remove, 0.1, entropy, 48, 201.347, 73.45
10, 0.5, False, remove, 0.1, depth, 48, 269.075, 60.26
10, 0.5, False, remove, 0.1, entropy, 48, 338.864, 60.64
10, 0.33, False, remove, 0.1, depth, 48, 407.072, 64.62
10, 0.33, False, remove, 0.1, entropy, 48, 476.811, 64.37
10, 0.75, False, remove, 0.1, depth, 48, 544.733, 55.11
10, 0.75, False, remove, 0.1, entropy, 48, 614.550, 55.64
10, 1.0, False, remove, 0.1, depth, 48, 682.197, 50.98
10, 1.0, False, remove, 0.1, entropy, 48, 752.083, 51.65
10, 3.0, False, remove, 0.1, depth, 48, 822.019, 32.72
10, 3.0, False, remove, 0.1, entropy, 48, 891.726, 34.03
10, 5.0, False, remove, 0.1, depth, 48, 959.456, 25.62
10, 5.0, False, remove, 0.1, entropy, 48, 1028.900, 26.86
1, 0.1, True, remove, 0.0, depth, 48, 57.963, 61.98
1, 0.1, True, remove, 0.0, entropy, 48, 104.965, 63.93
1, 0.5, True, remove, 0.0, depth, 48, 149.005, 43.79
1, 0.5, True, remove, 0.0, entropy, 48, 194.944, 49.29
1, 0.33, True, remove, 0.0, depth, 48, 238.992, 49.26
1, 0.33, True, remove, 0.0, entropy, 48, 285.210, 54.73
1, 0.75, True, remove, 0.0, depth, 48, 329.369, 38.99
1, 0.75, True, remove, 0.0, entropy, 48, 375.936, 44.13
1, 1.0, True, remove, 0.0, depth, 48, 420.057, 34.93
1, 1.0, True, remove, 0.0, entropy, 48, 467.304, 40.56
1, 3.0, True, remove, 0.0, depth, 48, 511.293, 23.04
1, 3.0, True, remove, 0.0, entropy, 48, 557.262, 27.11
1, 5.0, True, remove, 0.0, depth, 48, 601.934, 19.99
1, 5.0, True, remove, 0.0, entropy, 48, 647.909, 22.57
3, 0.1, True, remove, 0.0, depth, 48, 82.408, 70.01
3, 0.1, True, remove, 0.0, entropy, 48, 133.124, 70.61
3, 0.5, True, remove, 0.0, depth, 48, 181.886, 57.47
3, 0.5, True, remove, 0.0, entropy, 48, 232.582, 59.38
3, 0.33, True, remove, 0.0, depth, 48, 281.487, 61.77
3, 0.33, True, remove, 0.0, entropy, 48, 332.159, 63.75
3, 0.75, True, remove, 0.0, depth, 48, 380.874, 52.54
3, 0.75, True, remove, 0.0, entropy, 48, 432.688, 55.20
3, 1.0, True, remove, 0.0, depth, 48, 481.545, 48.05
3, 1.0, True, remove, 0.0, entropy, 48, 532.235, 51.05
3, 3.0, True, remove, 0.0, depth, 48, 581.292, 30.87
3, 3.0, True, remove, 0.0, entropy, 48, 633.100, 33.47
3, 5.0, True, remove, 0.0, depth, 48, 681.890, 24.63
3, 5.0, True, remove, 0.0, entropy, 48, 732.814, 26.24
5, 0.1, True, remove, 0.0, depth, 48, 89.882, 72.28
5, 0.1, True, remove, 0.0, entropy, 48, 144.298, 72.47
5, 0.5, True, remove, 0.0, depth, 48, 196.684, 60.98
5, 0.5, True, remove, 0.0, entropy, 48, 251.173, 61.90
5, 0.33, True, remove, 0.0, depth, 48, 303.502, 64.76
5, 0.33, True, remove, 0.0, entropy, 48, 357.923, 65.92
5, 0.75, True, remove, 0.0, depth, 48, 410.350, 55.72
5, 0.75, True, remove, 0.0, entropy, 48, 464.752, 57.47
5, 1.0, True, remove, 0.0, depth, 48, 517.157, 51.69
5, 1.0, True, remove, 0.0, entropy, 48, 571.580, 53.82
5, 3.0, True, remove, 0.0, depth, 48, 623.955, 33.22
5, 3.0, True, remove, 0.0, entropy, 48, 678.347, 35.54
5, 5.0, True, remove, 0.0, depth, 48, 730.616, 25.18
5, 5.0, True, remove, 0.0, entropy, 48, 785.045, 27.59
7, 0.1, True, remove, 0.0, depth, 48, 95.376, 73.28
7, 0.1, True, remove, 0.0, entropy, 48, 154.003, 73.36
7, 0.5, True, remove, 0.0, depth, 48, 210.623, 62.80
7, 0.5, True, remove, 0.0, entropy, 48, 269.182, 63.28
7, 0.33, True, remove, 0.0, depth, 48, 326.574, 66.21
7, 0.33, True, remove, 0.0, entropy, 48, 385.305, 66.33
7, 0.75, True, remove, 0.0, depth, 48, 441.878, 57.96
7, 0.75, True, remove, 0.0, entropy, 48, 500.477, 59.12
7, 1.0, True, remove, 0.0, depth, 48, 556.961, 54.17
7, 1.0, True, remove, 0.0, entropy, 48, 615.622, 55.50
7, 3.0, True, remove, 0.0, depth, 48, 672.179, 36.71
7, 3.0, True, remove, 0.0, entropy, 48, 730.717, 38.17
7, 5.0, True, remove, 0.0, depth, 48, 787.396, 27.91
7, 5.0, True, remove, 0.0, entropy, 48, 847.815, 29.54
10, 0.1, True, remove, 0.0, depth, 48, 124.303, 74.07
10, 0.1, True, remove, 0.0, entropy, 48, 189.006, 74.10
10, 0.5, True, remove, 0.0, depth, 48, 251.584, 63.72
10, 0.5, True, remove, 0.0, entropy, 48, 316.376, 64.37
10, 0.33, True, remove, 0.0, depth, 48, 378.912, 66.93
10, 0.33, True, remove, 0.0, entropy, 48, 443.795, 67.15
10, 0.75, True, remove, 0.0, depth, 48, 506.325, 59.12
10, 0.75, True, remove, 0.0, entropy, 48, 571.757, 60.10
10, 1.0, True, remove, 0.0, depth, 48, 635.935, 55.56
10, 1.0, True, remove, 0.0, entropy, 48, 700.455, 56.57
10, 3.0, True, remove, 0.0, depth, 48, 762.931, 37.90
10, 3.0, True, remove, 0.0, entropy, 48, 827.410, 39.88
10, 5.0, True, remove, 0.0, depth, 48, 889.861, 29.19
10, 5.0, True, remove, 0.0, entropy, 48, 954.366, 31.05
1, 0.1, True, remove, 0.25, depth, 48, 64.576, 62.73
1, 0.1, True, remove, 0.25, entropy, 48, 112.709, 64.62
1, 0.5, True, remove, 0.25, depth, 48, 159.060, 46.77
1, 0.5, True, remove, 0.25, entropy, 48, 207.317, 51.76
1, 0.33, True, remove, 0.25, depth, 48, 253.710, 51.76
1, 0.33, True, remove, 0.25, entropy, 48, 301.941, 56.42
1, 0.75, True, remove, 0.25, depth, 48, 348.172, 41.38
1, 0.75, True, remove, 0.25, entropy, 48, 397.214, 47.11
1, 1.0, True, remove, 0.25, depth, 48, 444.456, 37.70
1, 1.0, True, remove, 0.25, entropy, 48, 492.774, 43.24
1, 3.0, True, remove, 0.25, depth, 48, 539.054, 24.94
1, 3.0, True, remove, 0.25, entropy, 48, 587.247, 29.10
1, 5.0, True, remove, 0.25, depth, 48, 633.507, 21.96
1, 5.0, True, remove, 0.25, entropy, 48, 683.096, 24.68
3, 0.1, True, remove, 0.25, depth, 48, 91.280, 70.89
3, 0.1, True, remove, 0.25, entropy, 48, 143.795, 71.68
3, 0.5, True, remove, 0.25, depth, 48, 194.110, 58.26
3, 0.5, True, remove, 0.25, entropy, 48, 246.299, 60.51
3, 0.33, True, remove, 0.25, depth, 48, 296.598, 62.81
3, 0.33, True, remove, 0.25, entropy, 48, 348.897, 64.19
3, 0.75, True, remove, 0.25, depth, 48, 399.099, 52.40
3, 0.75, True, remove, 0.25, entropy, 48, 451.366, 54.94
3, 1.0, True, remove, 0.25, depth, 48, 501.609, 48.29
3, 1.0, True, remove, 0.25, entropy, 48, 553.811, 51.16
3, 3.0, True, remove, 0.25, depth, 48, 605.232, 31.69
3, 3.0, True, remove, 0.25, entropy, 48, 657.470, 34.31
3, 5.0, True, remove, 0.25, depth, 48, 707.833, 25.69
3, 5.0, True, remove, 0.25, entropy, 48, 760.046, 27.25
5, 0.1, True, remove, 0.25, depth, 48, 96.244, 72.64
5, 0.1, True, remove, 0.25, entropy, 48, 152.378, 73.13
5, 0.5, True, remove, 0.25, depth, 48, 206.504, 61.28
5, 0.5, True, remove, 0.25, entropy, 48, 264.666, 62.27
5, 0.33, True, remove, 0.25, depth, 48, 318.731, 65.00
5, 0.33, True, remove, 0.25, entropy, 48, 374.852, 65.84
5, 0.75, True, remove, 0.25, depth, 48, 429.058, 56.93
5, 0.75, True, remove, 0.25, entropy, 48, 485.065, 58.36
5, 1.0, True, remove, 0.25, depth, 48, 539.421, 53.00
5, 1.0, True, remove, 0.25, entropy, 48, 595.414, 54.87
5, 3.0, True, remove, 0.25, depth, 48, 649.568, 36.44
5, 3.0, True, remove, 0.25, entropy, 48, 705.681, 38.80
5, 5.0, True, remove, 0.25, depth, 48, 759.816, 28.62
5, 5.0, True, remove, 0.25, entropy, 48, 815.903, 30.93
7, 0.1, True, remove, 0.25, depth, 48, 102.756, 74.19
7, 0.1, True, remove, 0.25, entropy, 48, 163.043, 74.38
7, 0.5, True, remove, 0.25, depth, 48, 221.399, 62.80
7, 0.5, True, remove, 0.25, entropy, 48, 281.966, 63.88
7, 0.33, True, remove, 0.25, depth, 48, 340.310, 66.71
7, 0.33, True, remove, 0.25, entropy, 48, 400.790, 67.23
7, 0.75, True, remove, 0.25, depth, 48, 459.101, 58.13
7, 0.75, True, remove, 0.25, entropy, 48, 519.634, 59.30
7, 1.0, True, remove, 0.25, depth, 48, 578.027, 53.94
7, 1.0, True, remove, 0.25, entropy, 48, 638.271, 55.57
7, 3.0, True, remove, 0.25, depth, 48, 696.755, 35.86
7, 3.0, True, remove, 0.25, entropy, 48, 757.072, 37.81
7, 5.0, True, remove, 0.25, depth, 48, 816.735, 27.94
7, 5.0, True, remove, 0.25, entropy, 48, 877.053, 29.58
10, 0.1, True, remove, 0.25, depth, 48, 131.416, 75.09
10, 0.1, True, remove, 0.25, entropy, 48, 197.658, 75.27
10, 0.5, True, remove, 0.25, depth, 48, 261.858, 63.94
10, 0.5, True, remove, 0.25, entropy, 48, 328.126, 64.14
10, 0.33, True, remove, 0.25, depth, 48, 392.379, 67.08
10, 0.33, True, remove, 0.25, entropy, 48, 458.567, 67.29
10, 0.75, True, remove, 0.25, depth, 48, 522.762, 59.12
10, 0.75, True, remove, 0.25, entropy, 48, 588.832, 59.76
10, 1.0, True, remove, 0.25, depth, 48, 652.950, 55.41
10, 1.0, True, remove, 0.25, entropy, 48, 719.177, 56.37
10, 3.0, True, remove, 0.25, depth, 48, 783.558, 37.69
10, 3.0, True, remove, 0.25, entropy, 48, 849.712, 39.08
10, 5.0, True, remove, 0.25, depth, 48, 913.996, 29.64
10, 5.0, True, remove, 0.25, entropy, 48, 980.031, 30.94
1, 0.1, True, remove, 0.1, depth, 48, 61.604, 62.74
1, 0.1, True, remove, 0.1, entropy, 48, 108.912, 64.86
1, 0.5, True, remove, 0.1, depth, 48, 154.330, 45.70
1, 0.5, True, remove, 0.1, entropy, 48, 201.648, 51.16
1, 0.33, True, remove, 0.1, depth, 48, 247.771, 51.13
1, 0.33, True, remove, 0.1, entropy, 48, 295.176, 56.14
1, 0.75, True, remove, 0.1, depth, 48, 340.518, 40.70
1, 0.75, True, remove, 0.1, entropy, 48, 388.805, 45.64
1, 1.0, True, remove, 0.1, depth, 48, 434.195, 37.15
1, 1.0, True, remove, 0.1, entropy, 48, 481.450, 42.41
1, 3.0, True, remove, 0.1, depth, 48, 526.830, 23.53
1, 3.0, True, remove, 0.1, entropy, 48, 574.313, 27.35
1, 5.0, True, remove, 0.1, depth, 48, 619.697, 20.47
1, 5.0, True, remove, 0.1, entropy, 48, 667.194, 23.06
3, 0.1, True, remove, 0.1, depth, 48, 87.654, 70.86
3, 0.1, True, remove, 0.1, entropy, 48, 140.063, 71.62
3, 0.5, True, remove, 0.1, depth, 48, 190.510, 58.02
3, 0.5, True, remove, 0.1, entropy, 48, 242.906, 60.12
3, 0.33, True, remove, 0.1, depth, 48, 293.345, 62.86
3, 0.33, True, remove, 0.1, entropy, 48, 345.658, 64.40
3, 0.75, True, remove, 0.1, depth, 48, 396.001, 52.51
3, 0.75, True, remove, 0.1, entropy, 48, 448.393, 54.99
3, 1.0, True, remove, 0.1, depth, 48, 498.717, 48.18
3, 1.0, True, remove, 0.1, entropy, 48, 551.101, 51.31
3, 3.0, True, remove, 0.1, depth, 48, 601.433, 30.81
3, 3.0, True, remove, 0.1, entropy, 48, 653.850, 33.72
3, 5.0, True, remove, 0.1, depth, 48, 704.352, 24.47
3, 5.0, True, remove, 0.1, entropy, 48, 756.712, 26.70
5, 0.1, True, remove, 0.1, depth, 48, 93.689, 72.71
5, 0.1, True, remove, 0.1, entropy, 48, 149.351, 73.28
5, 0.5, True, remove, 0.1, depth, 48, 202.942, 61.24
5, 0.5, True, remove, 0.1, entropy, 48, 258.564, 62.45
5, 0.33, True, remove, 0.1, depth, 48, 312.250, 65.14
5, 0.33, True, remove, 0.1, entropy, 48, 367.928, 66.07
5, 0.75, True, remove, 0.1, depth, 48, 421.554, 56.57
5, 0.75, True, remove, 0.1, entropy, 48, 477.132, 58.14
5, 1.0, True, remove, 0.1, depth, 48, 530.788, 52.81
5, 1.0, True, remove, 0.1, entropy, 48, 586.420, 54.47
5, 3.0, True, remove, 0.1, depth, 48, 640.076, 34.67
5, 3.0, True, remove, 0.1, entropy, 48, 695.751, 37.51
5, 5.0, True, remove, 0.1, depth, 48, 749.370, 27.43
5, 5.0, True, remove, 0.1, entropy, 48, 805.048, 29.62
7, 0.1, True, remove, 0.1, depth, 48, 100.845, 74.41
7, 0.1, True, remove, 0.1, entropy, 48, 159.959, 74.71
7, 0.5, True, remove, 0.1, depth, 48, 217.027, 63.21
7, 0.5, True, remove, 0.1, entropy, 48, 276.079, 64.02
7, 0.33, True, remove, 0.1, depth, 48, 333.230, 66.68
7, 0.33, True, remove, 0.1, entropy, 48, 392.533, 67.39
7, 0.75, True, remove, 0.1, depth, 48, 449.585, 58.18
7, 0.75, True, remove, 0.1, entropy, 48, 508.775, 59.84
7, 1.0, True, remove, 0.1, depth, 48, 565.805, 53.94
7, 1.0, True, remove, 0.1, entropy, 48, 624.968, 55.74
7, 3.0, True, remove, 0.1, depth, 48, 682.042, 36.14
7, 3.0, True, remove, 0.1, entropy, 48, 741.029, 38.30
7, 5.0, True, remove, 0.1, depth, 48, 798.181, 27.23
7, 5.0, True, remove, 0.1, entropy, 48, 857.990, 29.27
10, 0.1, True, remove, 0.1, depth, 48, 129.949, 75.55
10, 0.1, True, remove, 0.1, entropy, 48, 194.257, 75.51
10, 0.5, True, remove, 0.1, depth, 48, 256.540, 64.20
10, 0.5, True, remove, 0.1, entropy, 48, 320.966, 64.59
10, 0.33, True, remove, 0.1, depth, 48, 383.223, 67.49
10, 0.33, True, remove, 0.1, entropy, 48, 447.561, 67.74
10, 0.75, True, remove, 0.1, depth, 48, 509.857, 59.48
10, 0.75, True, remove, 0.1, entropy, 48, 574.174, 60.32
10, 1.0, True, remove, 0.1, depth, 48, 636.466, 55.37
10, 1.0, True, remove, 0.1, entropy, 48, 700.727, 56.55
10, 3.0, True, remove, 0.1, depth, 48, 762.947, 36.84
10, 3.0, True, remove, 0.1, entropy, 48, 827.191, 38.38
10, 5.0, True, remove, 0.1, depth, 48, 889.628, 28.23
10, 5.0, True, remove, 0.1, entropy, 48, 953.829, 29.85
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': True, 'GAMMA': 0.1, 'NB_TRAIN_ITERATIONS': 10, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.1, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 48, 'TRAINING_TIME': 194.25689298845828, 'VALIDATION ACCURACY': 0.75507309622518}
-----TESTING
Final accuracy with best hyperparameters: 74.49
Mem in MB: 75.28
Mem in MB: 74.92
Mem in MB: 74.85
Mem in MB: 74.90
Mem in MB: 73.95
Mem in MB: 60.81
Mem in MB: 74.55
Mem in MB: 62.90
Mem in MB: 75.44
-----TIME PROFILING+
Read train + val data time:  1.04347
Number of training symbols: 73261665
Length of one training sequence: 999
Total training time: 25430.402 seconds
Number of test sequences: 9168
Length of test sequence: 999
Read test data time:  0.10797
Total inference time: 65.160 seconds
Inference time/symbol: 7.114455732981893e-06 seconds
-----MEMORY REPORT
Filename: /data/home/nsagan/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   229    138.5 MiB    138.5 MiB           1   @profile
   230                                         def main(dataset_folder, pretrain_file):
   231                                             global INCLUDE_PREV_CONTEXT
   232                                             global GAMMA
   233                                             global NB_TRAIN_ITERATIONS 
   234                                             global HANDLE_N_SETTING 
   235                                             global RATIO_PRETRAIN_TRAIN 
   236                                             global ENSEMBLE_TYPE 
   237                                             global NUM_THREADS
   238                                             
   239                                             global include_prev_contexts
   240                                             global gammas 
   241                                             global nb_train_iterations 
   242                                             global handle_N_settings 
   243                                             global ratio_pretrain_train
   244                                             global ensemble_type
   245                                             global num_threads
   246                                         
   247    138.5 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   248                                             
   249                                             # Read train, val, test data 
   250    138.5 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   251    138.5 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   252    138.5 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   253                                             
   254                                         
   255    211.4 MiB     73.0 MiB           1       train_data = pd.read_csv(train_path)
   256    219.3 MiB      7.8 MiB           1       validation_data = pd.read_csv(val_path)
   257                                             
   258    219.3 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   259    219.3 MiB      0.0 MiB           1       unique_labels = train_data['label'].unique()
   260                                             
   261    315.3 MiB      0.0 MiB           2       with open(pretrain_file, 'r') as file:
   262    315.3 MiB     96.1 MiB           1           pretrain_data = file.read()
   263                                             
   264                                             # Train all SPAs using all possible combinations of hyperparams
   265                                             # Test all on validation set, return best SPA
   266    315.3 MiB      0.0 MiB           1       results_df = pd.DataFrame(columns=[
   267                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   268                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   269                                             ])
   270                                         
   271    315.3 MiB      0.0 MiB           1       print("-----TRAINING")
   272    315.3 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   273    315.3 MiB      0.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   274    315.3 MiB      0.0 MiB           1       train_start_time = time.perf_counter()
   275   2460.6 MiB  -1836.5 MiB           8       for include_prev_context, handle_N_setting, ratio in itertools.product(
   276    315.3 MiB      0.0 MiB           1           include_prev_contexts, handle_N_settings, ratio_pretrain_train
   277                                             ):  
   278   2460.6 MiB  -1139.9 MiB           6           INCLUDE_PREV_CONTEXT = include_prev_context
   279   2460.6 MiB  -1139.9 MiB           6           GAMMA = gammas
   280   2460.6 MiB  -1139.9 MiB           6           NB_TRAIN_ITERATIONS = 0
   281   2460.6 MiB  -1139.9 MiB           6           HANDLE_N_SETTING = handle_N_setting
   282   2460.6 MiB  -1139.9 MiB           6           RATIO_PRETRAIN_TRAIN = ratio 
   283   2460.6 MiB  -1139.9 MiB           6           ENSEMBLE_TYPE = ensemble_type
   284   2460.6 MiB  -1139.9 MiB           6           NUM_THREADS = num_threads
   285                                                 
   286   2461.3 MiB  -1053.8 MiB           6           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   287   2461.3 MiB  -1139.9 MiB           6           nb_train_seqs = len(train_data)
   288   2461.3 MiB  -1139.9 MiB           6           seq_len = len(train_data.iloc[0, 0])
   289   2461.3 MiB  -1139.9 MiB           6           nb_train_symbols = nb_train_seqs * seq_len
   290                                                 
   291                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   292   2461.3 MiB -19428.2 MiB          72           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, compute_training_loss=False) for _ in unique_labels]
   293    783.2 MiB  -7355.6 MiB          60           for i in range(len(unique_labels)):
   294    783.2 MiB   -932.3 MiB         108               spa[i].set_inference_config(
   295    783.2 MiB   -466.2 MiB          54                   lb=1e-5,
   296    783.2 MiB   -466.2 MiB          54                   ensemble_type="entropy",
   297    783.2 MiB   -466.2 MiB          54                   ensemble_n=10,
   298    783.2 MiB   -466.2 MiB          54                   backshift_parsing=True,
   299    783.2 MiB   -466.2 MiB          54                   backshift_ctx_len=20,
   300    783.2 MiB   -466.2 MiB          54                   backshift_break_at_phrase=True
   301                                                     )
   302                                         
   303    783.2 MiB    -51.8 MiB           6           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   304   1127.3 MiB    903.3 MiB           6           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   305                                         
   306   1127.3 MiB   -930.8 MiB           6           iterated_times = 0
   307   2460.6 MiB -17072.6 MiB          36           for nb_iterations in nb_train_iterations:
   308   1752.2 MiB -20234.8 MiB          30               train_one_iter_start_time = time.perf_counter()
   309   2460.6 MiB -39904.3 MiB          90               for _ in range(nb_iterations - iterated_times):
   310   2460.6 MiB -28920.1 MiB          60                   train_spa_oneIter(train_data, spa)
   311                                                     
   312   2460.6 MiB -16143.0 MiB          30               iterated_times = nb_iterations
   313   2460.6 MiB -129136.0 MiB         240               for gamma in gammas:
   314   2460.6 MiB -338980.3 MiB         630                   for ensemble in ENSEMBLE_TYPE:
   315                                                         # Test on validation test to assess this combination of hyperparams
   316   2460.6 MiB -225984.5 MiB         420                       validation_data = handle_N(validation_data)
   317   2460.6 MiB -2259868.8 MiB        4200                       for index in range(len(spa)):
   318   2460.6 MiB -2033881.9 MiB        3780                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   319   2460.6 MiB -225907.0 MiB         420                       accuracy = test_seq(validation_data, spa, num_threads)
   320   2460.6 MiB -225986.0 MiB         420                       train_one_iter_end_time = time.perf_counter()
   321   2460.6 MiB -225986.0 MiB         420                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   322   2460.6 MiB -225986.0 MiB         420                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   323                                         
   324                                                         
   325                                                         
   326   2460.6 MiB -451972.0 MiB         840                       current_result = pd.DataFrame([{
   327   2460.6 MiB -225986.0 MiB         420                           "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   328   2460.6 MiB -225986.0 MiB         420                           "GAMMA": gamma,
   329   2460.6 MiB -225986.0 MiB         420                           "NB_TRAIN_ITERATIONS": nb_iterations,
   330   2460.6 MiB -225986.0 MiB         420                           "HANDLE_N_SETTING": HANDLE_N_SETTING,
   331   2460.6 MiB -225986.0 MiB         420                           "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   332   2460.6 MiB -225986.0 MiB         420                           "ENSEMBLE_TYPE": ensemble,
   333   2460.6 MiB -225986.0 MiB         420                           "NUM_THREADS": NUM_THREADS,
   334   2460.6 MiB -225986.0 MiB         420                           "TRAINING_TIME": train_one_iter_duration, 
   335   2460.6 MiB -225986.0 MiB         420                           "VALIDATION ACCURACY": accuracy
   336                                                             }])
   337                                         
   338                                                         # Concatenate the current result with results_df
   339   2460.6 MiB -112993.0 MiB         210                   results_df = results_df.dropna(axis=1, how='all')
   340   2460.6 MiB -112993.0 MiB         210                   current_result = current_result.dropna(axis=1, how='all')
   341                                         
   342   2460.6 MiB -112992.8 MiB         210                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   343                                         
   344                                             
   345                                             # Find the best hyperparameter combination based on the highest accuracy
   346   1764.0 MiB   -696.6 MiB           1       print("---BEST SPA(s) FOUND")
   347   1764.0 MiB      0.0 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   348   1764.0 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   349   1764.0 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   350                                         
   351                                             # Retrain and test using the best hyperparameters
   352   1764.0 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   353   1764.0 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   354   1764.0 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   355   1764.0 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   356   1764.0 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   357   1764.0 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   358   1764.0 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   359                                         
   360                                             # Retrain our best SPAs and use that to test on test data 
   361   1764.0 MiB   -985.5 MiB          12       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   362    778.4 MiB   -985.5 MiB          10       for i in range(len(unique_labels)):
   363    778.4 MiB      0.0 MiB          18           spa[i].set_inference_config(
   364    778.4 MiB      0.0 MiB           9               lb=1e-5,
   365    778.4 MiB      0.0 MiB           9               ensemble_type= ENSEMBLE_TYPE,
   366    778.4 MiB      0.0 MiB           9               ensemble_n=10,
   367    778.4 MiB      0.0 MiB           9               backshift_parsing=True,
   368    778.4 MiB      0.0 MiB           9               backshift_ctx_len=20,
   369    778.4 MiB      0.0 MiB           9               backshift_break_at_phrase=True
   370                                                 )
   371                                         
   372    778.4 MiB      0.0 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   373    778.4 MiB      0.0 MiB           1       nb_train_seqs = len(train_data)
   374    778.4 MiB      0.0 MiB           1       seq_len = len(train_data.iloc[0, 0])
   375    778.4 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   376    778.4 MiB      0.0 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   377                                         
   378    778.4 MiB      0.0 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   379   1764.0 MiB    985.5 MiB           1       train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   380                                         
   381   1764.0 MiB      0.0 MiB           1       train_end_time = time.perf_counter()
   382   1764.0 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   383                                         
   384                                             
   385                                             
   386                                             # Final test
   387   1764.0 MiB      0.0 MiB           1       print("-----TESTING")
   388   1764.0 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   389   1764.0 MiB      0.0 MiB           1       test_data = pd.read_csv(test_path)
   390                                         
   391   1764.0 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   392                                         
   393   1764.0 MiB      0.0 MiB           1       test_data = handle_N(test_data)
   394   1764.0 MiB      0.0 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   395                                         
   396   1764.0 MiB      0.0 MiB           1       inference_end_time = time.perf_counter()
   397   1764.0 MiB      0.0 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   398                                             
   399                                                 
   400   1764.0 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   401                                         
   402   1764.0 MiB      0.0 MiB           1       label = 0
   403   1839.4 MiB    -30.0 MiB          10       for sp in spa:
   404   1839.4 MiB     45.4 MiB           9           spa_bytes = bytearray(sp.to_bytes())
   405   1839.4 MiB    -30.0 MiB           9           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   406   1839.4 MiB    -30.0 MiB           9           makedirs("best_spas", exist_ok=True)
   407                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   408   1839.4 MiB    -30.0 MiB           9           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   409                                                 
   410                                                 # Create the full path for the binary file
   411   1839.4 MiB    -30.0 MiB           9           binary_file_path = os.path.join("best_spas", f"{binary_file_name}_{label}.bin")
   412   1839.4 MiB    -30.0 MiB           9           label += 1
   413                                                 # Save the binary file
   414   1839.4 MiB    -60.1 MiB          18           with open(binary_file_path, 'wb') as file:
   415   1839.4 MiB    -30.0 MiB           9               file.write(spa_bytes)
   416                                             
   417                                         
   418   1839.4 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   419   1839.4 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   420   1839.4 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   421   1839.4 MiB      0.0 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   422   1839.4 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   423                                             
   424                                         
   425   1839.4 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   426   1839.4 MiB      0.0 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   427   1839.4 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   428   1839.4 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   429   1839.4 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   430                                         
   431   1839.4 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


