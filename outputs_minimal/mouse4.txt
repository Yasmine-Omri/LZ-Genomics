-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, False, remove, 0.0, entropy, 64, 0.461, 61.18
1, 0.5, False, remove, 0.0, entropy, 64, 0.622, 64.90
1, 0.33, False, remove, 0.0, entropy, 64, 0.786, 63.30
1, 0.75, False, remove, 0.0, entropy, 64, 0.966, 65.32
1, 1.0, False, remove, 0.0, entropy, 64, 1.176, 65.53
1, 3.0, False, remove, 0.0, entropy, 64, 1.337, 68.45
1, 5.0, False, remove, 0.0, entropy, 64, 1.493, 69.09
3, 0.1, False, remove, 0.0, entropy, 64, 0.837, 63.30
3, 0.5, False, remove, 0.0, entropy, 64, 1.014, 64.42
3, 0.33, False, remove, 0.0, entropy, 64, 1.187, 63.62
3, 0.75, False, remove, 0.0, entropy, 64, 1.365, 65.27
3, 1.0, False, remove, 0.0, entropy, 64, 1.603, 66.22
3, 3.0, False, remove, 0.0, entropy, 64, 1.777, 68.45
3, 5.0, False, remove, 0.0, entropy, 64, 2.000, 69.62
5, 0.1, False, remove, 0.0, entropy, 64, 0.890, 64.21
5, 0.5, False, remove, 0.0, entropy, 64, 1.067, 65.59
5, 0.33, False, remove, 0.0, entropy, 64, 1.241, 64.84
5, 0.75, False, remove, 0.0, entropy, 64, 1.421, 66.17
5, 1.0, False, remove, 0.0, entropy, 64, 1.638, 66.44
5, 3.0, False, remove, 0.0, entropy, 64, 1.822, 68.61
5, 5.0, False, remove, 0.0, entropy, 64, 2.003, 69.94
7, 0.1, False, remove, 0.0, entropy, 64, 0.894, 61.60
7, 0.5, False, remove, 0.0, entropy, 64, 1.107, 64.37
7, 0.33, False, remove, 0.0, entropy, 64, 1.279, 63.57
7, 0.75, False, remove, 0.0, entropy, 64, 1.500, 65.16
7, 1.0, False, remove, 0.0, entropy, 64, 1.682, 66.06
7, 3.0, False, remove, 0.0, entropy, 64, 1.869, 68.56
7, 5.0, False, remove, 0.0, entropy, 64, 2.055, 69.20
10, 0.1, False, remove, 0.0, entropy, 64, 1.257, 62.83
10, 0.5, False, remove, 0.0, entropy, 64, 1.451, 64.84
10, 0.33, False, remove, 0.0, entropy, 64, 1.655, 64.10
10, 0.75, False, remove, 0.0, entropy, 64, 1.906, 65.22
10, 1.0, False, remove, 0.0, entropy, 64, 2.080, 65.59
10, 3.0, False, remove, 0.0, entropy, 64, 2.251, 68.35
10, 5.0, False, remove, 0.0, entropy, 64, 2.437, 68.61
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': False, 'GAMMA': 5.0, 'NB_TRAIN_ITERATIONS': 5, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.0, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 64, 'TRAINING_TIME': 2.0032601775601506, 'VALIDATION ACCURACY': 0.6994158258098778}
-----TESTING
Final accuracy with best hyperparameters: 68.83
Mem in MB: 10.29
Mem in MB: 10.11
-----TIME PROFILING+
Read train + val data time:  0.16186
Number of training symbols: 1521464
Length of one training sequence: 101
Total training time: 22.041 seconds
Number of test sequences: 1883
Length of test sequence: 101
Read test data time:  0.00792
Total inference time: 0.809 seconds
Inference time/symbol: 4.251625589225585e-06 seconds
-----MEMORY REPORT
Filename: /data/home/nsagan/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   229    137.8 MiB    137.8 MiB           1   @profile
   230                                         def main(dataset_folder, pretrain_file):
   231                                             global INCLUDE_PREV_CONTEXT
   232                                             global GAMMA
   233                                             global NB_TRAIN_ITERATIONS 
   234                                             global HANDLE_N_SETTING 
   235                                             global RATIO_PRETRAIN_TRAIN 
   236                                             global ENSEMBLE_TYPE 
   237                                             global NUM_THREADS
   238                                             
   239                                             global include_prev_contexts
   240                                             global gammas 
   241                                             global nb_train_iterations 
   242                                             global handle_N_settings 
   243                                             global ratio_pretrain_train
   244                                             global ensemble_type
   245                                             global num_threads
   246                                         
   247    137.8 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   248                                             
   249                                             # Read train, val, test data 
   250    137.8 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   251    137.8 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   252    137.8 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   253                                             
   254                                         
   255    142.0 MiB      4.2 MiB           1       train_data = pd.read_csv(train_path)
   256    141.9 MiB     -0.1 MiB           1       validation_data = pd.read_csv(val_path)
   257                                             
   258    141.9 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   259    141.9 MiB      0.0 MiB           1       unique_labels = train_data['label'].unique()
   260                                             
   261    237.9 MiB      0.0 MiB           2       with open(pretrain_file, 'r') as file:
   262    237.9 MiB     96.0 MiB           1           pretrain_data = file.read()
   263                                             
   264                                             # Train all SPAs using all possible combinations of hyperparams
   265                                             # Test all on validation set, return best SPA
   266    237.9 MiB      0.0 MiB           1       results_df = pd.DataFrame(columns=[
   267                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   268                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   269                                             ])
   270                                         
   271    237.9 MiB      0.0 MiB           1       print("-----TRAINING")
   272    237.9 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   273    237.9 MiB      0.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   274    237.9 MiB      0.0 MiB           1       train_start_time = time.perf_counter()
   275    318.3 MiB      0.0 MiB           3       for include_prev_context, handle_N_setting, ratio in itertools.product(
   276    237.9 MiB      0.0 MiB           1           include_prev_contexts, handle_N_settings, ratio_pretrain_train
   277                                             ):  
   278    237.9 MiB      0.0 MiB           1           INCLUDE_PREV_CONTEXT = include_prev_context
   279    237.9 MiB      0.0 MiB           1           GAMMA = gammas
   280    237.9 MiB      0.0 MiB           1           NB_TRAIN_ITERATIONS = 0
   281    237.9 MiB      0.0 MiB           1           HANDLE_N_SETTING = handle_N_setting
   282    237.9 MiB      0.0 MiB           1           RATIO_PRETRAIN_TRAIN = ratio 
   283    237.9 MiB      0.0 MiB           1           ENSEMBLE_TYPE = ensemble_type
   284    237.9 MiB      0.0 MiB           1           NUM_THREADS = num_threads
   285                                                 
   286    242.9 MiB      5.0 MiB           1           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   287    242.9 MiB      0.0 MiB           1           validation_data = handle_N(validation_data)
   288    242.9 MiB      0.0 MiB           1           nb_train_seqs = len(train_data)
   289    242.9 MiB      0.0 MiB           1           seq_len = len(train_data.iloc[0, 0])
   290    242.9 MiB      0.0 MiB           1           nb_train_symbols = nb_train_seqs * seq_len
   291                                                 
   292                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   293    242.9 MiB      0.0 MiB           5           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, compute_training_loss=False) for _ in unique_labels]
   294    242.9 MiB      0.0 MiB           3           for i in range(len(unique_labels)):
   295    242.9 MiB      0.0 MiB           4               spa[i].set_inference_config(
   296    242.9 MiB      0.0 MiB           2                   lb=1e-5,
   297    242.9 MiB      0.0 MiB           2                   ensemble_type="entropy",
   298    242.9 MiB      0.0 MiB           2                   ensemble_n=10,
   299    242.9 MiB      0.0 MiB           2                   backshift_parsing=True,
   300    242.9 MiB      0.0 MiB           2                   backshift_ctx_len=20,
   301    242.9 MiB      0.0 MiB           2                   backshift_break_at_phrase=True
   302                                                     )
   303                                         
   304    242.9 MiB      0.0 MiB           1           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   305    243.2 MiB      0.2 MiB           1           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   306                                         
   307    243.2 MiB      0.0 MiB           1           iterated_times = 0
   308    318.3 MiB     -0.5 MiB           6           for nb_iterations in nb_train_iterations:
   309    312.7 MiB      0.0 MiB           5               train_one_iter_start_time = time.perf_counter()
   310    318.5 MiB      0.0 MiB          15               for _ in range(nb_iterations - iterated_times):
   311    318.5 MiB     69.4 MiB          10                   train_spa_oneIter(train_data, spa)
   312                                                     
   313    318.5 MiB      0.0 MiB           5               iterated_times = nb_iterations
   314    318.5 MiB     -2.3 MiB          40               for gamma in gammas:
   315    318.5 MiB     -4.1 MiB          70                   for ensemble in ENSEMBLE_TYPE:
   316                                                         # Test on validation test to assess this combination of hyperparams
   317    318.5 MiB     -5.7 MiB         105                       for index in range(len(spa)):
   318    318.5 MiB     -3.8 MiB          70                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   319    318.5 MiB      3.0 MiB          35                       accuracy = test_seq(validation_data, spa, num_threads)
   320    318.5 MiB     -2.2 MiB          35                       train_one_iter_end_time = time.perf_counter()
   321    318.5 MiB     -2.2 MiB          35                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   322    318.5 MiB     -2.2 MiB          35                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   323                                         
   324                                                         
   325                                                         
   326    318.5 MiB     -4.4 MiB          70                       current_result = pd.DataFrame([{
   327    318.5 MiB     -2.2 MiB          35                           "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   328    318.5 MiB     -2.2 MiB          35                           "GAMMA": gamma,
   329    318.5 MiB     -2.2 MiB          35                           "NB_TRAIN_ITERATIONS": nb_iterations,
   330    318.5 MiB     -2.2 MiB          35                           "HANDLE_N_SETTING": HANDLE_N_SETTING,
   331    318.5 MiB     -2.2 MiB          35                           "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   332    318.5 MiB     -2.2 MiB          35                           "ENSEMBLE_TYPE": ensemble,
   333    318.5 MiB     -2.2 MiB          35                           "NUM_THREADS": NUM_THREADS,
   334    318.5 MiB     -2.2 MiB          35                           "TRAINING_TIME": train_one_iter_duration, 
   335    318.5 MiB     -2.2 MiB          35                           "VALIDATION ACCURACY": accuracy
   336                                                             }])
   337                                         
   338                                                         # Concatenate the current result with results_df
   339    318.5 MiB     -2.2 MiB          35                   results_df = results_df.dropna(axis=1, how='all')
   340    318.5 MiB     -2.0 MiB          35                   current_result = current_result.dropna(axis=1, how='all')
   341                                         
   342    318.5 MiB     -2.1 MiB          35                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   343                                         
   344                                             
   345                                             # Find the best hyperparameter combination based on the highest accuracy
   346    318.3 MiB      0.0 MiB           1       print("---BEST SPA(s) FOUND")
   347    318.3 MiB      0.0 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   348    318.3 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   349    318.3 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   350                                         
   351                                             # Retrain and test using the best hyperparameters
   352    318.3 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   353    318.3 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   354    318.3 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   355    318.3 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   356    318.3 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   357    318.3 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   358    318.3 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   359                                         
   360                                             # Retrain our best SPAs and use that to test on test data 
   361    318.3 MiB    -49.9 MiB           5       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   362    268.5 MiB    -49.9 MiB           3       for i in range(len(unique_labels)):
   363    268.5 MiB      0.0 MiB           4           spa[i].set_inference_config(
   364    268.5 MiB      0.0 MiB           2               lb=1e-5,
   365    268.5 MiB      0.0 MiB           2               ensemble_type= ENSEMBLE_TYPE,
   366    268.5 MiB      0.0 MiB           2               ensemble_n=10,
   367    268.5 MiB      0.0 MiB           2               backshift_parsing=True,
   368    268.5 MiB      0.0 MiB           2               backshift_ctx_len=20,
   369    268.5 MiB      0.0 MiB           2               backshift_break_at_phrase=True
   370                                                 )
   371                                         
   372    269.6 MiB      1.1 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   373    269.6 MiB      0.0 MiB           1       nb_train_seqs = len(train_data)
   374    269.6 MiB      0.0 MiB           1       seq_len = len(train_data.iloc[0, 0])
   375    269.6 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   376    269.6 MiB      0.0 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   377                                         
   378    269.6 MiB      0.0 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   379    299.8 MiB     30.2 MiB           1       train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   380                                         
   381    299.8 MiB      0.0 MiB           1       train_end_time = time.perf_counter()
   382    299.8 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   383                                         
   384                                             
   385                                             
   386                                             # Final test
   387    299.8 MiB      0.0 MiB           1       print("-----TESTING")
   388    299.8 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   389    299.8 MiB      0.0 MiB           1       test_data = pd.read_csv(test_path)
   390                                         
   391    299.8 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   392                                         
   393    299.8 MiB      0.0 MiB           1       test_data = handle_N(test_data)
   394    299.9 MiB      0.0 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   395                                         
   396    299.9 MiB      0.0 MiB           1       inference_end_time = time.perf_counter()
   397    299.9 MiB      0.0 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   398                                             
   399                                                 
   400    299.9 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   401                                         
   402    299.9 MiB      0.0 MiB           1       label = 0
   403    340.1 MiB      0.0 MiB           3       for sp in spa:
   404    340.1 MiB     40.2 MiB           2           spa_bytes = bytearray(sp.to_bytes())
   405    340.1 MiB      0.0 MiB           2           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   406    340.1 MiB      0.0 MiB           2           makedirs("best_spas", exist_ok=True)
   407                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   408    340.1 MiB      0.0 MiB           2           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   409                                                 
   410                                                 # Create the full path for the binary file
   411    340.1 MiB      0.0 MiB           2           binary_file_path = os.path.join("best_spas/minimal", f"{binary_file_name}_{label}.bin")
   412    340.1 MiB      0.0 MiB           2           label += 1
   413                                                 # Save the binary file
   414    340.1 MiB      0.0 MiB           4           with open(binary_file_path, 'wb') as file:
   415    340.1 MiB      0.0 MiB           2               file.write(spa_bytes)
   416                                             
   417                                         
   418    340.1 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   419    340.1 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   420    340.1 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   421    340.3 MiB      0.2 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   422    340.3 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   423                                             
   424                                         
   425    340.3 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   426    340.3 MiB      0.0 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   427    340.3 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   428    340.3 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   429    340.3 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   430                                         
   431    340.3 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


