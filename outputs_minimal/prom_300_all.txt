-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, False, remove, 0.0, entropy, 64, 3.454, 75.84
1, 0.5, False, remove, 0.0, entropy, 64, 4.589, 78.53
1, 0.33, False, remove, 0.0, entropy, 64, 5.750, 77.70
1, 0.75, False, remove, 0.0, entropy, 64, 6.896, 78.99
1, 1.0, False, remove, 0.0, entropy, 64, 8.081, 79.70
1, 3.0, False, remove, 0.0, entropy, 64, 9.212, 80.25
1, 5.0, False, remove, 0.0, entropy, 64, 10.375, 78.99
3, 0.1, False, remove, 0.0, entropy, 64, 6.561, 69.54
3, 0.5, False, remove, 0.0, entropy, 64, 7.857, 73.95
3, 0.33, False, remove, 0.0, entropy, 64, 9.126, 72.65
3, 0.75, False, remove, 0.0, entropy, 64, 10.399, 75.17
3, 1.0, False, remove, 0.0, entropy, 64, 11.708, 75.93
3, 3.0, False, remove, 0.0, entropy, 64, 13.042, 78.29
3, 5.0, False, remove, 0.0, entropy, 64, 14.340, 78.34
5, 0.1, False, remove, 0.0, entropy, 64, 7.195, 62.58
5, 0.5, False, remove, 0.0, entropy, 64, 8.500, 67.43
5, 0.33, False, remove, 0.0, entropy, 64, 9.855, 65.90
5, 0.75, False, remove, 0.0, entropy, 64, 11.179, 69.73
5, 1.0, False, remove, 0.0, entropy, 64, 12.568, 71.25
5, 3.0, False, remove, 0.0, entropy, 64, 13.900, 76.23
5, 5.0, False, remove, 0.0, entropy, 64, 15.277, 77.42
7, 0.1, False, remove, 0.0, entropy, 64, 7.543, 55.69
7, 0.5, False, remove, 0.0, entropy, 64, 8.919, 62.06
7, 0.33, False, remove, 0.0, entropy, 64, 10.278, 59.81
7, 0.75, False, remove, 0.0, entropy, 64, 11.681, 65.02
7, 1.0, False, remove, 0.0, entropy, 64, 13.018, 67.03
7, 3.0, False, remove, 0.0, entropy, 64, 14.435, 73.78
7, 5.0, False, remove, 0.0, entropy, 64, 15.796, 74.86
10, 0.1, False, remove, 0.0, entropy, 64, 10.972, 48.51
10, 0.5, False, remove, 0.0, entropy, 64, 12.414, 55.42
10, 0.33, False, remove, 0.0, entropy, 64, 13.869, 52.99
10, 0.75, False, remove, 0.0, entropy, 64, 15.257, 58.61
10, 1.0, False, remove, 0.0, entropy, 64, 16.729, 61.25
10, 3.0, False, remove, 0.0, entropy, 64, 18.139, 69.93
10, 5.0, False, remove, 0.0, entropy, 64, 19.577, 72.62
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': False, 'GAMMA': 3.0, 'NB_TRAIN_ITERATIONS': 1, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.0, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 64, 'TRAINING_TIME': 9.212033869698644, 'VALIDATION ACCURACY': 0.8025337837837838}
-----TESTING
Final accuracy with best hyperparameters: 78.72
Mem in MB: 19.48
Mem in MB: 18.75
-----TIME PROFILING+
Read train + val data time:  0.36222
Number of training symbols: 14206800
Length of one training sequence: 300
Total training time: 128.861 seconds
Number of test sequences: 5920
Length of test sequence: 300
Read test data time:  0.02378
Total inference time: 4.174 seconds
Inference time/symbol: 2.350091535665162e-06 seconds
-----MEMORY REPORT
Filename: /data/home/nsagan/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   229    138.0 MiB    138.0 MiB           1   @profile
   230                                         def main(dataset_folder, pretrain_file):
   231                                             global INCLUDE_PREV_CONTEXT
   232                                             global GAMMA
   233                                             global NB_TRAIN_ITERATIONS 
   234                                             global HANDLE_N_SETTING 
   235                                             global RATIO_PRETRAIN_TRAIN 
   236                                             global ENSEMBLE_TYPE 
   237                                             global NUM_THREADS
   238                                             
   239                                             global include_prev_contexts
   240                                             global gammas 
   241                                             global nb_train_iterations 
   242                                             global handle_N_settings 
   243                                             global ratio_pretrain_train
   244                                             global ensemble_type
   245                                             global num_threads
   246                                         
   247    138.0 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   248                                             
   249                                             # Read train, val, test data 
   250    138.0 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   251    138.0 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   252    138.0 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   253                                             
   254                                         
   255    156.8 MiB     18.8 MiB           1       train_data = pd.read_csv(train_path)
   256    159.0 MiB      2.3 MiB           1       validation_data = pd.read_csv(val_path)
   257                                             
   258    159.0 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   259    159.0 MiB      0.0 MiB           1       unique_labels = train_data['label'].unique()
   260                                             
   261    255.1 MiB      0.0 MiB           2       with open(pretrain_file, 'r') as file:
   262    255.1 MiB     96.0 MiB           1           pretrain_data = file.read()
   263                                             
   264                                             # Train all SPAs using all possible combinations of hyperparams
   265                                             # Test all on validation set, return best SPA
   266    255.1 MiB      0.0 MiB           1       results_df = pd.DataFrame(columns=[
   267                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   268                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   269                                             ])
   270                                         
   271    255.1 MiB      0.0 MiB           1       print("-----TRAINING")
   272    255.1 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   273    255.1 MiB      0.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   274    255.1 MiB      0.0 MiB           1       train_start_time = time.perf_counter()
   275    829.6 MiB      0.0 MiB           3       for include_prev_context, handle_N_setting, ratio in itertools.product(
   276    255.1 MiB      0.0 MiB           1           include_prev_contexts, handle_N_settings, ratio_pretrain_train
   277                                             ):  
   278    255.1 MiB      0.0 MiB           1           INCLUDE_PREV_CONTEXT = include_prev_context
   279    255.1 MiB      0.0 MiB           1           GAMMA = gammas
   280    255.1 MiB      0.0 MiB           1           NB_TRAIN_ITERATIONS = 0
   281    255.1 MiB      0.0 MiB           1           HANDLE_N_SETTING = handle_N_setting
   282    255.1 MiB      0.0 MiB           1           RATIO_PRETRAIN_TRAIN = ratio 
   283    255.1 MiB      0.0 MiB           1           ENSEMBLE_TYPE = ensemble_type
   284    255.1 MiB      0.0 MiB           1           NUM_THREADS = num_threads
   285                                                 
   286    268.6 MiB     13.5 MiB           1           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   287    267.6 MiB     -1.0 MiB           1           validation_data = handle_N(validation_data)
   288    267.6 MiB      0.0 MiB           1           nb_train_seqs = len(train_data)
   289    267.6 MiB      0.0 MiB           1           seq_len = len(train_data.iloc[0, 0])
   290    267.6 MiB      0.0 MiB           1           nb_train_symbols = nb_train_seqs * seq_len
   291                                                 
   292                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   293    267.6 MiB      0.0 MiB           5           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, compute_training_loss=False) for _ in unique_labels]
   294    267.6 MiB      0.0 MiB           3           for i in range(len(unique_labels)):
   295    267.6 MiB      0.0 MiB           4               spa[i].set_inference_config(
   296    267.6 MiB      0.0 MiB           2                   lb=1e-5,
   297    267.6 MiB      0.0 MiB           2                   ensemble_type="entropy",
   298    267.6 MiB      0.0 MiB           2                   ensemble_n=10,
   299    267.6 MiB      0.0 MiB           2                   backshift_parsing=True,
   300    267.6 MiB      0.0 MiB           2                   backshift_ctx_len=20,
   301    267.6 MiB      0.0 MiB           2                   backshift_break_at_phrase=True
   302                                                     )
   303                                         
   304    267.6 MiB      0.0 MiB           1           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   305    267.6 MiB      0.0 MiB           1           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   306                                         
   307    267.6 MiB      0.0 MiB           1           iterated_times = 0
   308    829.6 MiB     -0.3 MiB           6           for nb_iterations in nb_train_iterations:
   309    804.0 MiB      0.0 MiB           5               train_one_iter_start_time = time.perf_counter()
   310    829.6 MiB      0.0 MiB          15               for _ in range(nb_iterations - iterated_times):
   311    829.6 MiB    542.5 MiB          10                   train_spa_oneIter(train_data, spa)
   312                                                     
   313    829.6 MiB      0.0 MiB           5               iterated_times = nb_iterations
   314    829.6 MiB     -2.0 MiB          40               for gamma in gammas:
   315    829.6 MiB     -3.9 MiB          70                   for ensemble in ENSEMBLE_TYPE:
   316                                                         # Test on validation test to assess this combination of hyperparams
   317    829.6 MiB     -5.7 MiB         105                       for index in range(len(spa)):
   318    829.6 MiB     -3.8 MiB          70                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   319    829.6 MiB     16.9 MiB          35                       accuracy = test_seq(validation_data, spa, num_threads)
   320    829.6 MiB     -2.0 MiB          35                       train_one_iter_end_time = time.perf_counter()
   321    829.6 MiB     -2.0 MiB          35                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   322    829.6 MiB     -2.0 MiB          35                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   323                                         
   324                                                         
   325                                                         
   326    829.6 MiB     -3.7 MiB          70                       current_result = pd.DataFrame([{
   327    829.6 MiB     -2.0 MiB          35                           "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   328    829.6 MiB     -2.0 MiB          35                           "GAMMA": gamma,
   329    829.6 MiB     -2.0 MiB          35                           "NB_TRAIN_ITERATIONS": nb_iterations,
   330    829.6 MiB     -2.0 MiB          35                           "HANDLE_N_SETTING": HANDLE_N_SETTING,
   331    829.6 MiB     -2.0 MiB          35                           "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   332    829.6 MiB     -2.0 MiB          35                           "ENSEMBLE_TYPE": ensemble,
   333    829.6 MiB     -2.0 MiB          35                           "NUM_THREADS": NUM_THREADS,
   334    829.6 MiB     -2.0 MiB          35                           "TRAINING_TIME": train_one_iter_duration, 
   335    829.6 MiB     -2.0 MiB          35                           "VALIDATION ACCURACY": accuracy
   336                                                             }])
   337                                         
   338                                                         # Concatenate the current result with results_df
   339    829.6 MiB     -2.2 MiB          35                   results_df = results_df.dropna(axis=1, how='all')
   340    829.6 MiB     -1.9 MiB          35                   current_result = current_result.dropna(axis=1, how='all')
   341                                         
   342    829.6 MiB     -2.0 MiB          35                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   343                                         
   344                                             
   345                                             # Find the best hyperparameter combination based on the highest accuracy
   346    829.6 MiB      0.0 MiB           1       print("---BEST SPA(s) FOUND")
   347    829.6 MiB      0.0 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   348    829.6 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   349    829.6 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   350                                         
   351                                             # Retrain and test using the best hyperparameters
   352    829.6 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   353    829.6 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   354    829.6 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   355    829.6 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   356    829.6 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   357    829.6 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   358    829.6 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   359                                         
   360                                             # Retrain our best SPAs and use that to test on test data 
   361    829.6 MiB   -491.3 MiB           5       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   362    338.3 MiB   -491.3 MiB           3       for i in range(len(unique_labels)):
   363    338.3 MiB      0.0 MiB           4           spa[i].set_inference_config(
   364    338.3 MiB      0.0 MiB           2               lb=1e-5,
   365    338.3 MiB      0.0 MiB           2               ensemble_type= ENSEMBLE_TYPE,
   366    338.3 MiB      0.0 MiB           2               ensemble_n=10,
   367    338.3 MiB      0.0 MiB           2               backshift_parsing=True,
   368    338.3 MiB      0.0 MiB           2               backshift_ctx_len=20,
   369    338.3 MiB      0.0 MiB           2               backshift_break_at_phrase=True
   370                                                 )
   371                                         
   372    353.7 MiB     15.4 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   373    353.7 MiB      0.0 MiB           1       nb_train_seqs = len(train_data)
   374    353.7 MiB      0.0 MiB           1       seq_len = len(train_data.iloc[0, 0])
   375    353.7 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   376    353.7 MiB      0.0 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   377                                         
   378    353.7 MiB      0.0 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   379    398.5 MiB     44.8 MiB           1       train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   380                                         
   381    398.5 MiB      0.0 MiB           1       train_end_time = time.perf_counter()
   382    398.5 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   383                                         
   384                                             
   385                                             
   386                                             # Final test
   387    398.5 MiB      0.0 MiB           1       print("-----TESTING")
   388    398.5 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   389    398.7 MiB      0.2 MiB           1       test_data = pd.read_csv(test_path)
   390                                         
   391    398.7 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   392                                         
   393    398.7 MiB      0.0 MiB           1       test_data = handle_N(test_data)
   394    398.7 MiB     -0.0 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   395                                         
   396    398.7 MiB      0.0 MiB           1       inference_end_time = time.perf_counter()
   397    398.7 MiB      0.0 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   398                                             
   399                                                 
   400    398.7 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   401                                         
   402    398.7 MiB      0.0 MiB           1       label = 0
   403    456.4 MiB      0.0 MiB           3       for sp in spa:
   404    456.4 MiB     57.7 MiB           2           spa_bytes = bytearray(sp.to_bytes())
   405    456.4 MiB      0.0 MiB           2           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   406    456.4 MiB      0.0 MiB           2           makedirs("best_spas", exist_ok=True)
   407                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   408    456.4 MiB      0.0 MiB           2           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   409                                                 
   410                                                 # Create the full path for the binary file
   411    456.4 MiB      0.0 MiB           2           binary_file_path = os.path.join("best_spas/minimal", f"{binary_file_name}_{label}.bin")
   412    456.4 MiB      0.0 MiB           2           label += 1
   413                                                 # Save the binary file
   414    456.4 MiB      0.0 MiB           4           with open(binary_file_path, 'wb') as file:
   415    456.4 MiB      0.0 MiB           2               file.write(spa_bytes)
   416                                             
   417                                         
   418    456.4 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   419    456.4 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   420    456.4 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   421    456.4 MiB      0.0 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   422    456.4 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   423                                             
   424                                         
   425    456.4 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   426    456.4 MiB      0.0 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   427    456.4 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   428    456.4 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   429    456.4 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   430                                         
   431    456.4 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


