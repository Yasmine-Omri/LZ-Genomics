-----TRAINING
---SEARCH FOR BEST SPA(s)
nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy
1, 0.1, False, remove, 0.0, entropy, 64, 47.276, 63.67
1, 0.5, False, remove, 0.0, entropy, 64, 81.427, 49.41
1, 0.33, False, remove, 0.0, entropy, 64, 115.690, 54.71
1, 0.75, False, remove, 0.0, entropy, 64, 149.555, 44.56
1, 1.0, False, remove, 0.0, entropy, 64, 183.832, 40.92
1, 3.0, False, remove, 0.0, entropy, 64, 217.651, 25.74
1, 5.0, False, remove, 0.0, entropy, 64, 251.726, 22.49
3, 0.1, False, remove, 0.0, entropy, 64, 72.951, 69.92
3, 0.5, False, remove, 0.0, entropy, 64, 110.416, 58.03
3, 0.33, False, remove, 0.0, entropy, 64, 147.621, 62.37
3, 0.75, False, remove, 0.0, entropy, 64, 185.116, 53.24
3, 1.0, False, remove, 0.0, entropy, 64, 222.494, 49.41
3, 3.0, False, remove, 0.0, entropy, 64, 259.675, 32.51
3, 5.0, False, remove, 0.0, entropy, 64, 297.137, 26.12
5, 0.1, False, remove, 0.0, entropy, 64, 80.597, 71.79
5, 0.5, False, remove, 0.0, entropy, 64, 121.806, 59.69
5, 0.33, False, remove, 0.0, entropy, 64, 162.704, 63.96
5, 0.75, False, remove, 0.0, entropy, 64, 203.465, 55.06
5, 1.0, False, remove, 0.0, entropy, 64, 244.126, 50.77
5, 3.0, False, remove, 0.0, entropy, 64, 285.158, 33.67
5, 5.0, False, remove, 0.0, entropy, 64, 325.790, 26.35
7, 0.1, False, remove, 0.0, entropy, 64, 84.158, 72.06
7, 0.5, False, remove, 0.0, entropy, 64, 129.122, 60.45
7, 0.33, False, remove, 0.0, entropy, 64, 174.329, 63.81
7, 0.75, False, remove, 0.0, entropy, 64, 220.111, 55.77
7, 1.0, False, remove, 0.0, entropy, 64, 265.276, 51.51
7, 3.0, False, remove, 0.0, entropy, 64, 310.061, 34.25
7, 5.0, False, remove, 0.0, entropy, 64, 355.070, 26.86
10, 0.1, False, remove, 0.0, entropy, 64, 114.660, 73.06
10, 0.5, False, remove, 0.0, entropy, 64, 167.123, 60.70
10, 0.33, False, remove, 0.0, entropy, 64, 219.578, 64.74
10, 0.75, False, remove, 0.0, entropy, 64, 271.700, 55.98
10, 1.0, False, remove, 0.0, entropy, 64, 324.077, 52.06
10, 3.0, False, remove, 0.0, entropy, 64, 376.339, 35.06
10, 5.0, False, remove, 0.0, entropy, 64, 428.872, 28.15
---BEST SPA(s) FOUND
Best hyperparameters: {'INCLUDE_PREV_CONTEXT': False, 'GAMMA': 0.1, 'NB_TRAIN_ITERATIONS': 10, 'HANDLE_N_SETTING': 'remove', 'RATIO_PRETRAIN_TRAIN': 0.0, 'ENSEMBLE_TYPE': 'entropy', 'NUM_THREADS': 64, 'TRAINING_TIME': 114.659957931377, 'VALIDATION ACCURACY': 0.7306349552694742}
-----TESTING
Final accuracy with best hyperparameters: 72.20
Mem in MB: 54.74
Mem in MB: 54.44
Mem in MB: 54.46
Mem in MB: 54.37
Mem in MB: 53.56
Mem in MB: 41.10
Mem in MB: 54.05
Mem in MB: 43.06
Mem in MB: 55.09
-----TIME PROFILING+
Read train + val data time:  1.04695
Number of training symbols: 73261665
Length of one training sequence: 999
Total training time: 2015.625 seconds
Number of test sequences: 9168
Length of test sequence: 999
Read test data time:  0.10913
Total inference time: 63.275 seconds
Inference time/symbol: 6.9086834333099e-06 seconds
-----MEMORY REPORT
Filename: /data/home/nsagan/LZ-Genomics/Train.py

Line #    Mem usage    Increment  Occurrences   Line Contents
=============================================================
   229    137.9 MiB    137.9 MiB           1   @profile
   230                                         def main(dataset_folder, pretrain_file):
   231                                             global INCLUDE_PREV_CONTEXT
   232                                             global GAMMA
   233                                             global NB_TRAIN_ITERATIONS 
   234                                             global HANDLE_N_SETTING 
   235                                             global RATIO_PRETRAIN_TRAIN 
   236                                             global ENSEMBLE_TYPE 
   237                                             global NUM_THREADS
   238                                             
   239                                             global include_prev_contexts
   240                                             global gammas 
   241                                             global nb_train_iterations 
   242                                             global handle_N_settings 
   243                                             global ratio_pretrain_train
   244                                             global ensemble_type
   245                                             global num_threads
   246                                         
   247    137.9 MiB      0.0 MiB           1       read_data_in_time = time.perf_counter()
   248                                             
   249                                             # Read train, val, test data 
   250    137.9 MiB      0.0 MiB           1       train_path = f"{dataset_folder}/train.csv"
   251    137.9 MiB      0.0 MiB           1       val_path = f"{dataset_folder}/dev.csv"
   252    137.9 MiB      0.0 MiB           1       test_path = f"{dataset_folder}/test.csv"
   253                                             
   254                                         
   255    210.8 MiB     72.9 MiB           1       train_data = pd.read_csv(train_path)
   256    218.6 MiB      7.7 MiB           1       validation_data = pd.read_csv(val_path)
   257                                             
   258    218.6 MiB      0.0 MiB           1       ALPHABET_SIZE = 4
   259    218.6 MiB      0.0 MiB           1       unique_labels = train_data['label'].unique()
   260                                             
   261    314.6 MiB      0.0 MiB           2       with open(pretrain_file, 'r') as file:
   262    314.6 MiB     96.0 MiB           1           pretrain_data = file.read()
   263                                             
   264                                             # Train all SPAs using all possible combinations of hyperparams
   265                                             # Test all on validation set, return best SPA
   266    314.6 MiB      0.0 MiB           1       results_df = pd.DataFrame(columns=[
   267                                             "INCLUDE_PREV_CONTEXT", "GAMMA", "NB_TRAIN_ITERATIONS", 
   268                                             "HANDLE_N_SETTING", "RATIO_PRETRAIN_TRAIN", "ENSEMBLE_TYPE", "NUM_THREADS", "VALIDATION ACCURACY"
   269                                             ])
   270                                         
   271    314.6 MiB      0.0 MiB           1       print("-----TRAINING")
   272    314.6 MiB      0.0 MiB           1       print("---SEARCH FOR BEST SPA(s)")
   273    314.6 MiB      0.0 MiB           1       print("nb_iterations , gamma, include_prev_context, handle_N_setting, ratio, ensemble type, num_threads, time taken, accuracy", flush=True)
   274    314.6 MiB      0.0 MiB           1       train_start_time = time.perf_counter()
   275   1413.7 MiB      0.0 MiB           3       for include_prev_context, handle_N_setting, ratio in itertools.product(
   276    314.6 MiB      0.0 MiB           1           include_prev_contexts, handle_N_settings, ratio_pretrain_train
   277                                             ):  
   278    314.6 MiB      0.0 MiB           1           INCLUDE_PREV_CONTEXT = include_prev_context
   279    314.6 MiB      0.0 MiB           1           GAMMA = gammas
   280    314.6 MiB      0.0 MiB           1           NB_TRAIN_ITERATIONS = 0
   281    314.6 MiB      0.0 MiB           1           HANDLE_N_SETTING = handle_N_setting
   282    314.6 MiB      0.0 MiB           1           RATIO_PRETRAIN_TRAIN = ratio 
   283    314.6 MiB      0.0 MiB           1           ENSEMBLE_TYPE = ensemble_type
   284    314.6 MiB      0.0 MiB           1           NUM_THREADS = num_threads
   285                                                 
   286    395.1 MiB     80.5 MiB           1           train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   287    395.8 MiB      0.7 MiB           1           validation_data = handle_N(validation_data)
   288    395.8 MiB      0.0 MiB           1           nb_train_seqs = len(train_data)
   289    395.8 MiB      0.0 MiB           1           seq_len = len(train_data.iloc[0, 0])
   290    395.8 MiB      0.0 MiB           1           nb_train_symbols = nb_train_seqs * seq_len
   291                                                 
   292                                                 # Create list of spas based on number of labels: (spa_0 and spa_1 for labels 0, 1)
   293    396.0 MiB      0.2 MiB          12           spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, compute_training_loss=False) for _ in unique_labels]
   294    396.0 MiB      0.0 MiB          10           for i in range(len(unique_labels)):
   295    396.0 MiB      0.0 MiB          18               spa[i].set_inference_config(
   296    396.0 MiB      0.0 MiB           9                   lb=1e-5,
   297    396.0 MiB      0.0 MiB           9                   ensemble_type="entropy",
   298    396.0 MiB      0.0 MiB           9                   ensemble_n=10,
   299    396.0 MiB      0.0 MiB           9                   backshift_parsing=True,
   300    396.0 MiB      0.0 MiB           9                   backshift_ctx_len=20,
   301    396.0 MiB      0.0 MiB           9                   backshift_break_at_phrase=True
   302                                                     )
   303                                         
   304    396.0 MiB      0.0 MiB           1           nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   305    396.0 MiB      0.0 MiB           1           pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   306                                         
   307    396.0 MiB      0.0 MiB           1           iterated_times = 0
   308   1413.7 MiB     -0.0 MiB           6           for nb_iterations in nb_train_iterations:
   309   1044.9 MiB      0.0 MiB           5               train_one_iter_start_time = time.perf_counter()
   310   1413.7 MiB      0.0 MiB          15               for _ in range(nb_iterations - iterated_times):
   311   1413.7 MiB    937.1 MiB          10                   train_spa_oneIter(train_data, spa)
   312                                                     
   313   1413.7 MiB      0.0 MiB           5               iterated_times = nb_iterations
   314   1413.7 MiB     -0.1 MiB          40               for gamma in gammas:
   315   1413.7 MiB     -0.1 MiB          70                   for ensemble in ENSEMBLE_TYPE:
   316                                                         # Test on validation test to assess this combination of hyperparams
   317   1413.7 MiB     -0.7 MiB         350                       for index in range(len(spa)):
   318   1413.7 MiB     -0.6 MiB         315                           spa[index].set_inference_config(gamma=gamma, ensemble_type=ensemble)
   319   1413.7 MiB     80.2 MiB          35                       accuracy = test_seq(validation_data, spa, num_threads)
   320   1413.7 MiB     -0.1 MiB          35                       train_one_iter_end_time = time.perf_counter()
   321   1413.7 MiB     -0.1 MiB          35                       train_one_iter_duration = train_one_iter_end_time - train_one_iter_start_time
   322   1413.7 MiB     -0.1 MiB          35                       print(f"{nb_iterations}, {gamma}, {include_prev_context}, {handle_N_setting}, {ratio}, {ensemble}, {NUM_THREADS}, {train_one_iter_duration:.3f}, {(accuracy * 100):.2f}", flush=True)
   323                                         
   324                                                         
   325                                                         
   326   1413.7 MiB     -0.1 MiB          70                       current_result = pd.DataFrame([{
   327   1413.7 MiB     -0.1 MiB          35                           "INCLUDE_PREV_CONTEXT": INCLUDE_PREV_CONTEXT,
   328   1413.7 MiB     -0.1 MiB          35                           "GAMMA": gamma,
   329   1413.7 MiB     -0.1 MiB          35                           "NB_TRAIN_ITERATIONS": nb_iterations,
   330   1413.7 MiB     -0.1 MiB          35                           "HANDLE_N_SETTING": HANDLE_N_SETTING,
   331   1413.7 MiB     -0.1 MiB          35                           "RATIO_PRETRAIN_TRAIN": RATIO_PRETRAIN_TRAIN,
   332   1413.7 MiB     -0.1 MiB          35                           "ENSEMBLE_TYPE": ensemble,
   333   1413.7 MiB     -0.1 MiB          35                           "NUM_THREADS": NUM_THREADS,
   334   1413.7 MiB     -0.1 MiB          35                           "TRAINING_TIME": train_one_iter_duration, 
   335   1413.7 MiB     -0.1 MiB          35                           "VALIDATION ACCURACY": accuracy
   336                                                             }])
   337                                         
   338                                                         # Concatenate the current result with results_df
   339   1413.7 MiB      0.2 MiB          35                   results_df = results_df.dropna(axis=1, how='all')
   340   1413.7 MiB     -0.1 MiB          35                   current_result = current_result.dropna(axis=1, how='all')
   341                                         
   342   1413.7 MiB     -0.0 MiB          35                   results_df = pd.concat([results_df, current_result], ignore_index=True)
   343                                         
   344                                             
   345                                             # Find the best hyperparameter combination based on the highest accuracy
   346   1413.7 MiB      0.0 MiB           1       print("---BEST SPA(s) FOUND")
   347   1413.7 MiB      0.0 MiB           1       best_row = results_df.loc[results_df['VALIDATION ACCURACY'].idxmax()]
   348   1413.7 MiB      0.0 MiB           1       best_params = best_row.to_dict()
   349   1413.7 MiB      0.0 MiB           1       print("Best hyperparameters:", best_params)
   350                                         
   351                                             # Retrain and test using the best hyperparameters
   352   1413.7 MiB      0.0 MiB           1       INCLUDE_PREV_CONTEXT = best_params["INCLUDE_PREV_CONTEXT"]
   353   1413.7 MiB      0.0 MiB           1       GAMMA = best_params["GAMMA"]
   354   1413.7 MiB      0.0 MiB           1       NB_TRAIN_ITERATIONS = int(best_params["NB_TRAIN_ITERATIONS"])
   355   1413.7 MiB      0.0 MiB           1       HANDLE_N_SETTING = best_params["HANDLE_N_SETTING"]
   356   1413.7 MiB      0.0 MiB           1       RATIO_PRETRAIN_TRAIN = best_params["RATIO_PRETRAIN_TRAIN"]
   357   1413.7 MiB      0.0 MiB           1       ENSEMBLE_TYPE = best_params["ENSEMBLE_TYPE"]
   358   1413.7 MiB      0.0 MiB           1       NUM_THREADS = best_params["NUM_THREADS"]
   359                                         
   360                                             # Retrain our best SPAs and use that to test on test data 
   361   1413.7 MiB   -937.2 MiB          12       spa = [LZ78SPA(alphabet_size=ALPHABET_SIZE, gamma= GAMMA, compute_training_loss=False) for _ in unique_labels]
   362    476.5 MiB   -937.2 MiB          10       for i in range(len(unique_labels)):
   363    476.5 MiB      0.0 MiB          18           spa[i].set_inference_config(
   364    476.5 MiB      0.0 MiB           9               lb=1e-5,
   365    476.5 MiB      0.0 MiB           9               ensemble_type= ENSEMBLE_TYPE,
   366    476.5 MiB      0.0 MiB           9               ensemble_n=10,
   367    476.5 MiB      0.0 MiB           9               backshift_parsing=True,
   368    476.5 MiB      0.0 MiB           9               backshift_ctx_len=20,
   369    476.5 MiB      0.0 MiB           9               backshift_break_at_phrase=True
   370                                                 )
   371                                         
   372    478.2 MiB      1.7 MiB           1       train_data = handle_N(train_data, setting=HANDLE_N_SETTING)
   373    478.2 MiB      0.0 MiB           1       nb_train_seqs = len(train_data)
   374    478.2 MiB      0.0 MiB           1       seq_len = len(train_data.iloc[0, 0])
   375    478.2 MiB      0.0 MiB           1       nb_train_symbols = nb_train_seqs * seq_len
   376    478.2 MiB      0.0 MiB           1       nb_pretrain_symbols = math.ceil(RATIO_PRETRAIN_TRAIN * nb_train_symbols)
   377                                         
   378    478.2 MiB      0.0 MiB           1       pretrain_spa(pretrain_data, spa, nb_pretrain_symbols) 
   379   1443.4 MiB    965.2 MiB           1       train_spa(train_data, spa, iterations=NB_TRAIN_ITERATIONS)
   380                                         
   381   1443.4 MiB      0.0 MiB           1       train_end_time = time.perf_counter()
   382   1443.4 MiB      0.0 MiB           1       train_duration = train_end_time - train_start_time
   383                                         
   384                                             
   385                                             
   386                                             # Final test
   387   1443.4 MiB      0.0 MiB           1       print("-----TESTING")
   388   1443.4 MiB      0.0 MiB           1       read_test_data_start_time = time.perf_counter()
   389   1443.4 MiB      0.0 MiB           1       test_data = pd.read_csv(test_path)
   390                                         
   391   1443.4 MiB      0.0 MiB           1       inference_start_time = time.perf_counter()
   392                                         
   393   1443.5 MiB      0.0 MiB           1       test_data = handle_N(test_data)
   394   1443.7 MiB      0.3 MiB           1       test_accuracy = test_seq(test_data, spa, NUM_THREADS)
   395                                         
   396   1443.7 MiB      0.0 MiB           1       inference_end_time = time.perf_counter()
   397   1443.7 MiB      0.0 MiB           1       print(f"Final accuracy with best hyperparameters: {(test_accuracy*100):.2f}")
   398                                             
   399                                                 
   400   1443.7 MiB      0.0 MiB           1       inference_duration = inference_end_time - inference_start_time
   401                                         
   402   1443.7 MiB      0.0 MiB           1       label = 0
   403   1498.8 MiB    -28.2 MiB          10       for sp in spa:
   404   1498.8 MiB     26.9 MiB           9           spa_bytes = bytearray(sp.to_bytes())
   405   1498.8 MiB    -28.2 MiB           9           print(f"Mem in MB: {len(spa_bytes) / (1024 * 1024):.2f}", flush=True)
   406   1498.8 MiB    -28.2 MiB           9           makedirs("best_spas", exist_ok=True)
   407                                                 # Extract the part after 'GUE/' and replace slashes with underscores
   408   1498.8 MiB    -28.2 MiB           9           binary_file_name = dataset_folder.split("GUE/", 1)[-1].replace("/", "_")
   409                                                 
   410                                                 # Create the full path for the binary file
   411   1498.8 MiB    -28.2 MiB           9           binary_file_path = os.path.join("best_spas/minimal", f"{binary_file_name}_{label}.bin")
   412   1498.8 MiB    -28.2 MiB           9           label += 1
   413                                                 # Save the binary file
   414   1498.8 MiB    -56.4 MiB          18           with open(binary_file_path, 'wb') as file:
   415   1498.8 MiB    -28.2 MiB           9               file.write(spa_bytes)
   416                                             
   417                                         
   418   1498.8 MiB      0.0 MiB           1       print("-----TIME PROFILING+")
   419   1498.8 MiB      0.0 MiB           1       print(f"Read train + val data time: {(train_start_time - read_data_in_time): .5f}")
   420   1498.8 MiB      0.0 MiB           1       print(f"Number of training symbols: {nb_train_symbols}")
   421   1498.8 MiB      0.0 MiB           1       print(f"Length of one training sequence: {len(train_data.iloc[0, 0])}")
   422   1498.8 MiB      0.0 MiB           1       print(f"Total training time: {train_duration:.3f} seconds")
   423                                             
   424                                         
   425   1498.8 MiB      0.0 MiB           1       print(f"Number of test sequences: {len(test_data)}")
   426   1498.8 MiB      0.0 MiB           1       print(f"Length of test sequence: {len(test_data.iloc[0, 0])}")
   427   1498.8 MiB      0.0 MiB           1       print(f"Read test data time: {(inference_start_time - read_test_data_start_time): .5f}")
   428   1498.8 MiB      0.0 MiB           1       print(f"Total inference time: {inference_duration:.3f} seconds")
   429   1498.8 MiB      0.0 MiB           1       print(f"Inference time/symbol: {inference_duration/(len(test_data) * len(test_data.iloc[0, 0]))} seconds")
   430                                         
   431   1498.8 MiB      0.0 MiB           1       print("-----MEMORY REPORT")


